@c uncomment the following two lines for 'update every node' command
@c @node  tut
@c @chapter Tutorial Introduction (using Bp)

In this chapter, @ref{tut-bp}, @ref{tut-using} and @ref{tut-config}
provide a tutorial introduction in three parts.  @ref{tut-bp} provides a
brief overview of a connectionist learning algorithm, backpropagation,
and a simple but classic learning problem that can be solved by this
model, called XOR.  It is worth reading this section even if you are
familiar with these problems, since it introduces some of the conceptual
structure behind the simulator and some important terminology, such as
the names of many of the variable names used in the simulator.
@ref{tut-using} shows you how to use the PDP++ system to run
backpropagation on the XOR example, indicating how to control the
learning process, modify options and parameters, save and load results,
etc.  @ref{tut-config} shows you how to to configure the backpropagation
model for an example of you own.  To illustrate the configuration
process we choose another classic problem, the Encoder problem.

@menu
* tut-bp::                      Backpropagation and XOR
* tut-using::                   Using the Simulator to run Bp on the XOR Example
* tut-config::                  Configuring the Encoder Problem
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-bp, tut-using, tut, tut
@section Backpropagation and XOR

Backpropagation is the name of a learning algorithm used to train
multi-layer networks of simple processing units (Rumelhart, Hinton &
Williams, 1986).  In the simple case we consider in this tutorial, we
restrict our attention to multi-layer feed-forward networks.  Such
networks consist of several layers of units, the first (one or more)
of which are the input layer(s) and the last of which are the output
layer(s).  Each layer consists of some number of simple connectionist
units.  Units in lower-numbered layers may send connections to units
in any higher-numbered layer, but in feedforward networks they cannot
send connections to units in the same layer or lower-numbered layers.

The network learns from events it is exposed to by its training
environment.  Each event consists of an input pattern for each input
layer and a target output pattern for each output layer.  Henceforth
we will consider the case of a single input and output layer.  The
goal of learning is to adjust the weights on the connections among the
units so as to allow the network to produce the target output pattern
in response to the given input pattern.  Weights changes are based on
calculating the derivative of the error in the network's output with
respect to each weight.  

Training is organized into a series of epochs.  In each epoch, the
network is exposed to a set of events, often the entire set of events
that comprise the training environment.  for each event, processing
occurs in three phases: an activation phase, a back-propagation phase,
and a final phase in which weight error derivatives are calculated.

Training begins by initializing the weights and biases to small random
values. The process of learning then begins, and continues for some
number of epochs or until a performance criterion is reached.
Typically this criterion is given in terms of the total,
summed over all of the events in the epoch, of some measure of
performance on each event; the sum squared error, described below,
is the most frequently used measure.

We now consider in each of the three phases involved in processing
each pattern.  

@menu
* tut-bp-act::                  Activation Phase
* tut-bp-backprop::             BackPropagation Phase
* tut-bp-weight::               Weight Error Derivative Phase
* tut-bp-xor::                  
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-bp-act, tut-bp-backprop, tut-bp, tut-bp
@subsection Activation Phase

In this phase, the activations of the input units are set to the values
given in the input pattern, and activation is propagated forward through
the network to set the activations of units at successively
higher-numbered layers, ending with the output layer.  The idea is that
the connection weights determine the outcome of this process, and the
purpose of this pass is to determine how closely this outcome matches
the target.  During this pass, we calculate two quantities for each
unit: First the net input it receives from the units that project to it,
and second the activation, based on the net input.  The net input is
simply the sum, over all of the incoming connections to the unit, of the
weight of the connection times the activation of the sending unit, plus
a bias term:

@iftex
@tex
% html net_i = SUM_i a_j w_ij + b_i 
$$   net_i = \sum_i a_j w_{ij} + b_i $$	
@end tex
@end iftex
@ifinfo
     net_i = SUM_i a_j w_ij + b_i 
@end ifinfo

This sum may run over all of the units in all of the lower-numbered
layers, but the connectivity may be restricted by design.  the term
w_ij signifies the weight to unit i from unit j, and the term b_i
signifies the bias associated with unit i.  The activation is simply a
monotonically increasing sigmoidal function of the net input.  The
simulator uses the logistic function:

@iftex
@tex
% html a_i = (1 / (1 + e^(-net_i)))
     $$ a_i =  {1 \over {1 + e^{-net_i}}}  $$
@end tex
@end iftex

@ifinfo
@example
	            1 
        a_i =  --------------
               1 + exp(-net_i)

@end example
@end ifinfo


The activation ranges from 0 to 1 as the net input ranges from minus
infinity to infinity.  As an option other ranges can be used, in
which case the function becomes

@iftex
@tex
% html a_i = min + ((max - min) * (1/(1+ exp(-net_i))));
$$     a_i  = { min +{{(max - min)} { 1 \over {1 + e^{-net_i}}}}} $$
@end tex
@end iftex

@ifinfo
@example
	a_i = min + (max - min)          1          
                                 ---------------------
                                    1 + exp(-net_i)
@end example
@end ifinfo

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-bp-backprop, tut-bp-weight, tut-bp-act, tut-bp
@subsection BackPropagation Phase

In this phase, a measure of the error or mismatch between the target and
the actual output is computed, and the negative of the derivative of the
error with respect to the activation and with respect to the net input
to each unit is computed, starting at the output layer and passing
successively back through successively lower numbers of layers.  By
default the error measure used is the summed square error:

@iftex
@tex
% html 	ss = SUM_i (t_i - a_i)^2
$$	ss = \sum_i (t_i - a_i)^2   $$
@end tex
@end iftex

@ifinfo
@example
	ss = SUM_i (t_i - a_i)^2

@end example
@end ifinfo


where the index i runs over all of the output units.  The derivative
of this with respect to the activation of each output unit is easily
computed.  We actually use the negative of this derivative, which we
call @i{dE/da_i}.  For output units, this quantity is

@iftex
@tex
% html dE/da_i = (t_i - a_i)
$$     {dE \over da_i} = {t_i - a_i}  $$
@end tex
@end iftex

@ifinfo
	dE/da_i = (t_i - a_i)
@end ifinfo


We then compute a quantity proportional to the negative derivative of
the error measure with respect to the net input to each unit, which is

@iftex
@tex
% html dE/dnet_i = dE/da_i f'(net_i)
$$ 	{ dE \over dnet_i} = {dE \over da_i} {f'(net_i)} $$
@end tex
@end iftex

@ifinfo
	dE/dnet_i = dE/da_i f'(net_i)
@end ifinfo

where @i{f'(net_i)} is the slope of the activation function given the net
input to the unit.

This quantity is then propagated back to successively lower numbered
layers, using the formulas

@iftex
@tex
% html dE/da_j = SUM_i w_ij dE/dnet_i
$$	{dE \over da_j} = \sum_i w_{ij} {dE \over dnet_i}   $$
@end tex
@end iftex
and
@sp 1
@iftex
@tex
% html dE/dnet_j = dE/da_j f'(net_j)
$$ 	{ dE \over dnet_j} = {dE \over da_j} {f'(net_j)}  $$
@end tex
@end iftex

@ifinfo
	dE/da_j = SUM_i w_ij dE/dnet_i
and
	dE/dnet_j = dE/da_j f'(net_j)
@end ifinfo


Here subscript @i{i} indexes units that receive connections from unit @i{j};
we can see these formulas as providing for the backward propagation of
error derivative information via the same connections that carry
activation forward during the activation phase.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-bp-weight, tut-bp-xor, tut-bp-backprop, tut-bp
@subsection Weight Error Derivative Phase

In this phase, we calculate quantities called @i{dE/dw_ij} and
@i{dE/db_i} for all of the weights and all of the biases in the network.
These quantities again represent the negative derivative of the Error
with respect to the weights and the biases.  The formulae are:

@iftex
@tex
% html dE/dw_ij = dE/dnet_i * a_j 
$$ {dE \over dw_{ij}} = {{ dE \over dnet_i} * a_j}  $$
@end tex
@end iftex
and
@sp 1
@iftex
@tex
% html dE/db_i = dE/dnet_i
$$ {dE \over db_i} = {dE \over dnet_i} $$
@end tex
@end iftex
@ifinfo
@example
        dE/dw_ij = dE/dnet_i * a_j  

   and

        dE/db_i = dE/dnet_i
@end example
@end ifinfo


After these error derivatives are calculated, there are two options.
They may be summed over an entire epoch of training, or they may be used
to change the weights immediately.  In the latter case, the variable
@i{dE/dw_ij} accumulates the value of this derivative over all of the patterns
processed within the epoch, and similarly for @i{dE/db_i}.

When the weight change is made, there are two further values 
calculated.  First, the exact magnitude of the change to be made is
calculated:

@iftex
@tex
% html  deltaW_ij = lrate * dE/dw_ij + momentum * deltaW_ij
$$ \Delta W_{ij} = {({lrate * {dE \over dw_ij}}) + {momentum * deltaW_{ij}}}  $$
@end tex
@end iftex

@ifinfo
        deltaW_ij = lrate * dE/dw_ij + momentum * deltaW_ij
@end ifinfo

This expression introduces a learning rate parameter, which scales the
size of the effect of the weight error derivative, and a momentum
parameter, which scales the extent to which previous weight changes
carry through to the current weight change.  Note that in the expression
for @i{deltaW_ij} the value of @i{deltaW_ij} from the previous time step
appears on the right hand side.  An analogous expression is used to
calculate the @i{deltaBias_i}.  Finally, the weight change is actually
applied:

@iftex
@tex
% html w_ij += deltaW_ij
$$   w_{ij} = \Delta W_{ij} $$
@end tex
@end iftex

@ifinfo
        w_ij += deltaW_ij
@end ifinfo

Once again, an analogous expression governs the updating of @i{bias_i}.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-bp-xor,  , tut-bp-weight, tut-bp
@subsection The XOR Problem

XOR is about the simplest problem requiring a hidden
layer of units between the inputs and the outputs.  For this reason,
it has become something of a paradigm example for the back propagation
learning algorithm.  The problem is to learn a set of weights that
takes two-element input patterns, where each element is a 1 or a 0,
and computes the exclusive-or (XOR) of these inputs.  XOR is a boolean
function which takes on the value '1' if either of the input bits is a
1, but not if both are 1:

@example
@group
Table [XOREvents]:

       Input  Output

        0 0     0
        0 1     1
        1 0     1
        1 1     0
@end group
@end example

The output should be 1 if one of the inputs is 1; if neither is 1 or
if both is 1, the output should be 0. 

The environment for learning XOR consists of four events, one for each
of the input-output pairs that specify the XOR function.   In the
standard training regime, each pattern is presented once per epoch,
and training continues until the total squared error, summed across
all four input-output pairs falls below a criterion value.  The value
is usually set to something like .04.  With this criterion, all the
none of the cases can be off by more than about .2.

Various network configurations can be used, but for this example we will
use the configuration shown in Figure [XORNet].  In this configuration, the
input layer consists of two units, one for each input element; the
output layer consists of a single unit, for the result of the XOR
computation; and the hidden layer consists of two units.  The output
unit receives connections from both hidden units and each hidden unit
receives connections from both input units.  The hidden and output
units also have modifiable biases, shown in the figure as with
arrows labeled with 'b' coming in from the side.

@example
@group
Figure [XORNet]:

           Output Unit <- b
            ^      ^
            |       \
            |        \
            ^         ^
    b ->   Left      Right  <- b
           Hidden   Hidden
            ^    ^  ^    ^   
            |     \/     |
            |     /\     |
            |    /  \    |
            ^   ^    ^   ^
           Left      Right
           Input     Input

@end group
@end example

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using, tut-config, tut-bp, tut
@section  Using the Simulator to run BP on the XOR Example

The brief description of back propagation and XOR just given in
section BP introduces key concepts and terminology, including the
actual names of the variables used in the simulator.  With that 
background, you are ready to turn to the simulator itself.

@menu
* tut-using-notation::          Notation for Tutorial Instructions
* tex-using-starting::          Starting up PDP++
* tut-using-hierarchy::         Object Hierarchy: The Root Object and the Project
* tut-using-network::           Network and NetView Window
* tut-using-environment::       The Environment
* tut-using-processes::         Processes
* tut-using-logs::              Logging Training and Testing Data
* tut-using-running::           Running the Processes
* tut-using-monitoring::        Monitoring Network Variables
* tut-using-changing::          Changing Things
* tut-using-saving::            Saving and restoring.
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-notation, tex-using-starting, tut-using, tut-using
@subsection Notation for Tutorial Instructions

Before we go any further, we will need a few notational conventions, so
let us introduce some.  We use @kbd{underlined} type for things that you
will type.  We use @code{typewriter} for labels and prompts, such as the
prompt in the PDP++ shell or a label on a type-in menu boxes.  @b{bold}
is used to denote an object type name, like @b{BpUnit}.  We use
@i{italic} for menu items or buttons you will click on or select using
the SELECT (usually left) mouse key.  For multi-step menu selections we
will use statements like 'select @i{A/B/C}'.  This means that you should
press and hold the select key on menu item @i{A}.  When a popup menu
appears move the pointer to item @i{B}.  When a further popup menu
appears, move the pointer to @i{C}, and then release the select key.
Finally, we use double quotes to designate strings that name objects
such as files, layers, units, etc.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tex-using-starting, tut-using-hierarchy, tut-using-notation, tut-using
@subsection Starting up PDP++

The XOR demo project that we will be using is located in a directory
where the PDP++ software was installed.  You should go to that directory
(typically @file{/usr/local/pdp++}) and then change to the
@file{demo/bp} directory, which is where the project files are.

To start using the back propagation package within the PDP++ system, you
simply type @kbd{bp++} to your unix command interface.  After a short
time, the @code{bp++>} prompt will appear in your command interface
window, and a small menubar window will appear somewhere on the screen.
This is the @code{PDP++:Root} window, and it contains three menus.  The
first is called @i{Object}, the second is called @i{.projects}, and the
third is called @i{.colorspecs}.  The first menu's name alerts you to
the fact that this window is associated with an object, whose name
appears as the label on the window.  The other menu names refer to the
children of Root, which consist of projects and colorspecs.  You can
think of the '.' in front of the names as indicating that the projects
and colorspecs are children of the current object.

You are going to open a pre-defined @b{Project} and then run it.  To
open the project, select @i{.projects} / @i{Open In} / @i{Root}.  A
file-selector window will appear.  It gives a list of directories
accessible from your current directory and files in the current
directory that might contain a project.  You can either type the name of
the project file in the @code{enter filename} box, or double click on an
item in the the list to select it.  We want the project file
"xor.proj.gz". This file is saved in a compressed format, which is why
it has the ".gz" extension.  Double click on it, and the project will be
loaded in (the simulator automatically uncompresses files stored in the
".gz" compression format).

After a little time for initialization, the Root menu will move to the
right, and you should see two new windows on your screen: A @b{Project}
window and a window called a @b{NetView}.  Several other windows will be
iconified, we will turn our attention to them in a moment.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-hierarchy, tut-using-network, tex-using-starting, tut-using
@subsection Object Hierarchy: The Root Object and the Project

As a result of loading the "xor.proj" file, we created a number of
objects, all of which can be thought of as organized into a hierarchical
tree, with Root at the top of the tree.  The Root window is associated
with the Root object, which is a generic object that basically just
serves as a place holder for other objects.  We opened the XOR project
in Root, and so an object representing the entire XOR project has been
created as a child of Root.  The @b{Project} window is associated with
this project; in fact we could create a second project under root, and
if we did a second project window would be created.  Typically, though,
we just have one project.  Within this project, we have both a network
and an environment, as well as some other things we will consider later
in this tutorial.  Within the network we have layers of units; within
the units we have projections from other layers, which contain
connections.

Each object can be identified by a string that specifies its location in
the object hierarchy.  Root sits at the top of the object hierarchy; our
project is a 'child' of root.  It's identifier string is
@code{.projects[0]}.  In general, the identifier string for an object
specifies its location in the object hierarchy as a pathway
specification that begins with a '.', followed by fields separated by
dots.  Each field contains a type-name and an index.  One can trace the
pathway to a particular object down from Root (implicit in the first
dot) through intermediate fields separated by dots, ending with the
field specifying the type and token number of the object itself.  The
path for the project is short since it is the first (and only) project
that is a child of Root.  This is all exactly like a directory
hierarchy, except we have objects instead of files and the separator is
'.' instead of '/'.

Objects can have proper names as well as type names and indices; these
names can be used to access the object directly and to give it a
mnemonic label that can be used in referring to it.  The windows you see
give the full identifier string and if a name has been assigned, they
give the name as well in parentheses.

Within the Project window, we have several menus accessible.  As before,
one of these menus refers to the Project Object itself (the @i{Object}
menu).  The other menus refer to the children of the project,
which are again organized into several types.  The main ones we will
care about are the networks, the environments, the processes, and the
logs.  The defaults, specs, and scripts are for customizations that we
will not consider for the moment.  We could access the network through
the @i{.networks} menu, but we already have a window up on the screen
that gives us access to the network, called the @b{NetView}.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-network, tut-using-environment, tut-using-hierarchy, tut-using
@subsection Network and NetView Window

The @b{NetView} window (@pxref{net-view}) is an essential window for us
since it provides us with access to the network that we have created for
learning the XOR problem.  The @b{NetView} window has two sets of menus
along with lots of pushbuttons and a display area containing a graphical
display of the network. Lets take a look at the menus first.  The left
hand set of menus relate to the @b{Network} itself.  The right hand set
of menus relate to the @b{NetView}.  The @b{NetView} is an object,
actually a child of the @b{Network}, that provides a view of the
Network.  We can operate on either one, and we have two separate sets of
menus.

The left hand menus allow access to the @b{Network} itself and the
Networks' children: its @i{.views} and its @i{.layers}.  Most of these
menus have the same basic structure as we have already seen, but there
is one new menu, namely the @i{Actions} menu.  This menu allows us to
perform actions on the network itself, such as removing things from it,
initializing it, etc.  Some of the other actions we can perform are
accessible through the buttons to which we will turn in the next
paragraph.  The right hand menus allow us to access the @b{NetView}.
This is indicated by the label @i{View:} on this set of menus.  These
menus operate on the visual display of the network, or any operation
that interacts with the visual display (e.g., the @i{Selections} menu
operates on things that are selected in the visual display).

The @b{NetView} itself, the main body of the NetView window, provides a
graphical depiction of our network.  You can think of the layers of the
network arranged vertically, with the units within each layer laid out
on a two-dimensional sheet.  The input layer is located at the bottom of
the netview, with the two input units arranged side by side.  In
networks with many units per layer the units can be laid out in an X by
Y array.  The hidden layer is next above the input layer, and we can see
the two hidden units, again laid out side by side.  The output layer,
above the hidden layer, contains just a single unit, aligned to the left
of the netview.  At present, each unit is displaying its activation, as
indicated by the depressed appearance of the @code{act} button, together
with the yellow highlight.  The activation is shown in color in
accordance with the color bar shown at the right of the NetView, and by
the numeric value printed on it.  Either way we see that the activation
of all of the units is 0.

The @b{Network} has been initialized so that each unit has a random bias
weight and so that there are random weights on the connections between
the units.  We can examine these biases and weights by displaying them
on the netview.  First, to look at the bias weights, we can click on the
@i{bias.wt} button.  Once we do this, the units will assume colors
corresponding to their bias weights.  The biases on the input units are
not used and can be ignored.

We can also view weights either coming into or going out of a selected
unit.  The former are called 'receiving weights'.  To view the receiving
weights of the left hidden unit we first click on the button labeled
@i{r.wt}.  This activates the @i{View} button, and turns the cursor into
a little hand, indicating that we may now pick a unit to view its
receiving weights.  We can now click on the left hidden unit, and its
receiving or incoming weights are displayed on the units from which they
originate; the selected receiving unit is highlighted.  We see that this
unit receives incoming weights only from the two input units, both
somewhat positive.  We can proceed to click on other units to view their
incoming weights as well.  Note that the input units have no incoming
weights.  To see the outgoing or 'sending' weights from a unit, we
simply click the @i{s.wt} button, then click the unit whose sending
weights we wish to view.  You can verify that the values of the sending
weight from the left input unit to the right hidden unit is the same
value as the receiving weight to the right hidden unit.

Since all we've done so far is initialize the network's weights there
isn't that much else we can look at at this point.  However, if we click
on the @i{act} button, the network will be set to display the
activations of units later when we actually start to run the network.
Do this now.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-environment, tut-using-processes, tut-using-network, tut-using
@subsection The Environment

A second type of object contained in our Project is the @b{Environment}.
The environment can be thought of as a special object that supplies
events (training examples) on request.  In real life, if our network
were embedded in the brain of some organism the training experiences
would arise from the world.  In the simulator we will imagine that we
sample events from the environment when a process requests them.  In
general the environment can itself be an arbitrarily complex process,
but in the present case, the environment simply contains a list of
'events', each of which in turn consists of an input-output pattern
pair.  If we double click on the iconified @b{EnviroView} window, we can
inspect the objects corresponding to each of these Events.  The window
that pops up displays the names of all four of the events that make up
the 'XOR' problem.  You can inspect any one of these events by clicking
on the appropriate button, and if you do you will see that it consists
of two patterns, an input and an output pattern.  You can display all
four together, by first clicking the left mouse button on the first
event button, then clicking the middle mouse button on the other event
buttons in order.  The color scale indicates the correct interpretation
of the colors that indicate the activations of the units.  Once you
check this out you can go ahead and iconify the @b{EnviroView} window
again.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-processes, tut-using-logs, tut-using-environment, tut-using
@subsection Processes

Since everything has been properly initialized, and the various displays
are all laid out for our use, we are ready to try to teach the network
the XOR function.  We train networks, and test them, by running
@b{Process} objects that operate on them.

@b{Process} objects are described at length in @ref{proc}.  We will only
say enough about them here to give you a general sense of what they are
and what they do.  You can think of processes as objects that consist of
several parts: an initialization part, a loop part, and a finishing-up
part.  When a process is run, it first carries out its initialization
process; then it loops for a specified number of iterations or until
some performance criterion is met, and then it finishes up and exits.
For many processes, the loop consists of simply calling the process's
sub-process over and over again.  At the bottom of the process tree, we
call routines that actually interact with the network.  Processes also
spawn sub-processes that calculate statistics, such as the sum of
squares statistic that measures how well the network has solved the
learning problem that has been posed to it.  Some statistics, called
@code{loop_stats}, are calculated at the end of each step in the process
loop; others are calculated only when finishing up.

In the case of backpropagation, we have a three-level process hierarchy.
At the bottom of the hierarchy is the @b{TrialProcess}.  This process
takes an event, consisting of an input pattern and a target pattern,
from the @b{Environment}, and then carries out the three phase process
described previously (@pxref{tut-bp}).  The processing is actually
implemented by looping through the @b{Layer}s in ascending numerical
order; then looping, within each layer, through the @b{Unit}s, and
calling functions associated with each unit to compute the quantities as
previously described.  All of the variables mentioned there are
explicitly stored in each unit, and can be inspected once they have been
computed by selecting the appropriate button in the @b{NetView}.  At the
end of the trial, the sum of squares statistic is calculated; in this
case it isn't much of a sum since there is just one output unit.

The Trial process is actually called as a sub-process of the
@b{EpochProcess}.  All the Epoch process does is loop through the set of
pattern pairs in the @b{Environment}, calling the trial process to
process each pattern pair.  Before it starts, it initializes its
statistic, the sum of the trial-wise sum-squared-error
(@code{sum_sum_SE_Stat}), to 0. As it loops, it computes the
@code{sum_sum_SE_Stat} by aggregating the @code{sum_SE_Stat} statistic
over each of the training trails, so that at the end of the epoch, it
reflects the sum over all of the trials in the epoch.  At the end of the
epoch, it passes the @code{sum_sum_SE_Stat} up to the training process.

The Epoch process is called by the @b{TrainProcess}.  This process
initializes the epoch counter before it starts.  It stops when the
counter reaches a specified maximum value, or when the latest (or last)
sum-squared-error value (@code{lst_sum_SE_Stat}) falls below its
stopping criterion.  The stopping criterion is actually a property of
the Train process's @b{SE_Stat} statistic, @code{lst_sum_SE_Stat}.  This
statistic simply copies the last @code{sum_sum_SE_Stat} from the epoch
process, and compares it to the criterion, passing a special return
value to the Train process when the criterion is met.  If so, the
process exits from the loop at that point, and training is stopped.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-logs, tut-using-running, tut-using-processes, tut-using
@subsection Logging Training and Testing Data

Before we start to train our @b{Network}, and see if it can learn the
XOR problem, we will need a couple of tools for visualizing the
network's performance as it learns.  To allow for this, we have created
two objects called @b{Log}s, one for recording the state of various
network statistics and variables as they are computed during the
training process, and one for recording the activations of the hidden
and output units, when the network is tested with the learning process
switched off.  The data recorded in these logs are displayed in the
@b{GraphlogView} and the @b{GridLogView} respectively.  Double click on
both of these to open them up.

The @b{GraphLogView} is set to display the sum of the sum-squared-error
measure (@code{sum_sum_se}), summing over all output units and all of
the training patterns presented within each epoch.  The Epoch number
will be the X axis and the summed squared error will be the Y axis.
When we start to run the network this will be updated after every epoch
as we shall see.

The @b{GridLogView} is set to display detailed information about each
pattern whenever a special test process is run.  During the test, each
event in the environment is presented once, and the View displays the
epoch number, the Event name, the sum_squared error over all output
units (though in this case there is only one), and the activations of
the hidden and output units that occur when the input pattern associated
with this event is presented.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-running, tut-using-monitoring, tut-using-logs, tut-using
@subsection Running the Processes

To run a process, we first create a 'control panel' window for it.
Select @i{.processes} / @i{ControlPanel} / @i{Train} in the @b{Project}
window; this creates a control panel for running the @b{TrainProcess}.
At this point, if you were to click on @i{Run} (don't do it now!), the
network will proceed to run epochs, until either the stopping criterion
is met or @code{max} epochs (shown in the @code{max} typein box) are
completed.

Rather than start training right away, let's run through a test before
any training is carried out.  We can do this using the "Test" process,
which is actually just another @b{EpochProcess} that is set up to run
one epoch without learning.  Select @i{.processes} / @i{ControlPanel} /
@i{Test} in the @b{Project} window, then simply press @i{Step}.  Now,
you will see some action.  First, assuming that @i{act} is selected in
the @b{NetView}, you will see the activations of all of the units as the
first pattern is processed.  The input units are both off (0) in the
first pattern, and should have the color associated with 0 in the color
scale.  The hidden and output units all have activations between about
0.4 and 0.6.  The same information is displayed in the @b{GridLogView}.
You should see the epoch number, the Event name, the sum-squared error
for this event, and a display showing the activation of the hidden and
output units for this event.  If you press @i{Step} 3 more times you can
step through the remaining test patterns.  At the end of stepping you
should have the results from all four cases visible in the GridLogView.

Now let's run 30 epochs of training and see what happens.  To do this,
just click on @i{Step} in the "Train" control panel, which is pre-set to
run 30 epochs in each step (as indicated by the value of @code{n} in the
control panel @code{step} field).  When you do this, assuming that
indeed the @i{act} button has been clicked in the @b{NetView}, you will
see the state of activation in the network flicker through all four
patterns thirty times.  At the end of each epoch the total sum of
squares will be displayed in the @b{GraphLogView} --- the results are
pretty dismal at first, with a sum of sum-of-squares-error
(@code{sum_sum_se}) oscillating a bit very close to 1.

After 30 epochs you can run through another test if you want, either
stepping through the patterns one at a time as before with the @i{Step}
button or running a complete test of all of the items with the @i{Run}
button.  If you hit @i{Run} the results will flicker by rather fast in
the @b{NetView} but again all four cases will be logged in the
@b{GridLogView}.  You can see that the activations of the hidden and
output units are very similar across all four cases.

You can proceed to taking successive steps of 30 epochs, then testing,
if you like, or, if you prefer, you can simply let the learning process
run to criterion, and then run a test only at that point.  To do the
latter, just hit @i{Run} in the "Train" control panel.  You can click
off the @i{Display} toggle in the @b{NetView} (near the upper left) ---
this will speed things up a bit, even though the @b{GraphLogView} will
still be adding new data after each epoch.

As with the example of learning XOR from the original PDP handbook, not
much happens to the sum of sum-squared-error (@code{sum_sum_se}) until
about epoch 160 or so.  Now the error begins to drop, slowly at first,
then faster... until at about 270 epochs the stopping criterion is
reached, and the problem is solved.  Hit @i{Step} in the "Test" control
panel, to step through a test at this point.  You should be able to see
either in the @b{NetView} or in the @b{GridLogView} that the units that
should be off are close to off (less than about .1) and the units that
should be on are most of the way on (activation about .9).

If you would like to let the network learn to get the activations even
closer to the correct values, you'll need to change the stopping
criterion.  This criterion is associated with the sum of
sum-squared-error statistic (@code{lst_sum_SE_Stat}) in the
@b{TrainProcess}.  To access it, select @i{.processes} / @i{Edit} /
@i{Train} / @i{lst_sum_Train_SE_Stat} from the .project window.  The
label "lst_sum_Train_SE_Stat" indicates that this @b{SE_Stat}
(squared-error statistic) is associated with the @b{TrainProcess}.  It
originates at the trial level, but its @code{sum} over training trials
within the epoch is passed to the epoch level, and the @code{lst} (last,
latest) value at the epoch level is passed to the Train process.

An edit dialog box will come up when you @i{Edit} it.  This may seem a
bit daunting at this point --- we're letting you see how much there is
under the hood here --- but bear with us for a moment.  All you need to
do is find the row of this dialog labeled @code{se}.  You'll see its
current value, a toggle indicating that it is set to serve as a stopping
criterion, a criterion-type (here less-than-or-equal or @code{<=}), and
to the right of this a numeric criterion value (which should be 0.04).
Click in this numeric field, and set it to 0.01.

Before the new value takes effect, it must be propagated from the dialog
box to the actual internal state of the simulator.  As soon as you have
started to make change, the dialog will highlight the @i{Apply} button
to indicate that you need to press this button to apply any changes that
have been made to the state of the simulator.  Once you have the value
field showing the new value that you want to use, click @i{Apply} and
the change will be implemented.  Then click @i{Ok} to dismiss the window
Note that @i{Ok} also performs an "Apply" if necessary, so you can
actually just do this in one step.  However, often you'll want to keep a
dialog around after you make changes, which is where the @i{Apply}
button is useful.

Now hit @i{Run} again in the "Train" control panel, and the network will
run around 70 more epochs to get its performance to the new, more
stringent criterion.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-monitoring, tut-using-changing, tut-using-running, tut-using
@subsection Monitoring Network Variables

It is often useful to monitor the values of network variables, such as
the activation of the output units, in your log files.  For example, you
may wish to watch how the network gradually differentiates its output in
response to the four input patterns.  We can do this by creating what is
called a @b{MonitorStat} statistic.  

Conceptually, a monitor statistic is a special type of statistic that
simply monitors values already computed by the processes that are
applied to the network.  For example, activations are calculated by the
@b{TrialProcess} during training.  To record these activations in a log
we need to monitor them and that's what a Monitor Statistic will do.
Think of it as an electrode for recording data at specified times inside
your network.

To create a Monitor statistic, first select the units and the variable
you want to monitor in the @b{NetView}.  To monitor with activation of
the output unit, for example, we would make sure @code{act} is selected
as the variable, and then we would select the output unit.  In this case
we can select either the unit or the whole layer, it won't make any
difference.  To select the layer, make sure the @i{Select} button is
highlighted in the NetView and then click over the output layer itself.
The layer's border will be highlighted.  Now Select @i{Monitor Values} /
@i{New} (located in the middle-left region of the netview), and a pop-up
will appear with some necessary information displayed.

The popup indicates that you are creating a type of Statistic called a
@b{MonitorStat}, and it indicates which process will contain this
Statistic.  In this case, the @code{In Process} button will indicate
that the statistic will be placed at the end of the "TrainTrial"
process, which is what we want (we already have something monitoring
activations in the "TestTrial" process).  The @code{For Variable} should
be @code{act}.  The @code{Using Operator} in this case is @code{COPY},
which is also what we want -- we just want to copy the values of all the
selected units into the statistic.  The last aspect of this is
conceptually the most challenging.  What we need to do is to specify
that we want to @code{Create Aggregates} of this variable at higher
levels.  In this case, what we really want to do is just copy the
activation from all 4 patterns to the Epoch level, so we can display the
activations of the output unit for the different events in our Graph.
So click the @i{CreateAggregates} toggle, then click @i{Ok}. Another
popup will then appear, asking you what form of aggregation you want.
In this case you must select @i{COPY}, to indicate that we are copying
each event's output unit activation into the epoch graph, not averaging
or summing or anything like that.  Do this, click @i{Ok}, and you are
done.

Before you restart the simulation, you'll need to reinitialize the
weights.  We do this in the Train Control Panel.  You reinitialize with
the same starting weights by clicking @i{Re Init}, or you can initialize
with new starting weights by clicking @i{New Init}.  Either way, the
network is reinitialized and the epoch counter is set to 0.  Click @i{Re
Init} this time so we can see trace the learning process over the very
same trajectory as before.  To run the simulation again we can simply
press @i{Run} or @i{Step} in the "Train" process control panel.  You'll
see the new results graphed on the same graph along with the earlier sum
of sum-squared-error results.

The graph may seem a little confusing at first.  Overlaying the
@code{sum_sum_se} results will be four new lines, which will seem to be
oscillating rather extremely.  Buttons identifying these new lines will
be shown on the left of the View.  Note that the color of the graph line
associated with a variable that is displayed is shown to the left of the
Button, and the color of the axis that is associated with the variable
is displayed on the right.  All four new lines are associated with a new
Y-axis, which you can see covers a rather narrow range of values (about
.46 to .54).  This axis is tied to the entire group of activations that
are being monitored, and it is auto-scaled to the largest and smallest
value in the display.

You may want to fix the scale to something reasonable like 0-1.  To do
this, click with the @emph{right} mouse button --- the @i{Edit} button
-- on the lead (top) element of the set of new elements (it may be
labeled @code{output.act} or @code{cpy_output.act_0}.  A popup will
appear.  You want to set @code{min mode} and @code{max mode} to
@code{FIXED} so click where it says @code{AUTO GROUP} and select
@code{FIXED}, then enter 1.0 in the max field next to @code{range}.  To
make this take effect, click @i{Update View}, then click @i{Ok}.  The
new Y axis will now span the 0-1 range, and the oscillations will appear
more moderate on this scale.  This axis adjustment can be done while the
simulation is running.

In any case, you'll see that after thirty epochs or so the oscillations
level off.  The simulation appears to be flatlined until about epoch
120, when the activations of the output unit begin to differentiate the
four input patterns. (The outputs to patterns 1 and 2, which are the 01
and 10 patterns, are nearly identical and the latter lies on top of the
former much of the time).  In any case, you should see that the network
tends to produce the strongest output for the '11' pattern (pattern 3)
until about epoch 210, well after it has completely nailed the '00'
case.  After epoch 210 the performance changes rapidly for the 10, 01,
and 11 patterns (patterns 1, 2, and 3) until about epoch 270 or so,
where they begin to level off at their correct values.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-changing, tut-using-saving, tut-using-monitoring, tut-using
@subsection Changing Things

Our network has succeeded in learning XOR, but none too quickly.  The
question arises, can backpropagation do a better job?  The first thing
that might occur to you to try is increasing the learning rate, so
let's try that.

Before you can change the learning rate, you must understand how such
variables are handled in the simulator.  The simulator is set up so that
it is possible to have different learning rates for different weights.
This would, for example, allow us to have one learning rate for the
weights from the input layer to the hidden layer, and a different
learning rate for the weights from the hidden layer to the output layer.
To allow this, parameters such as the learning rate are stored in
structures called @emph{specifications} or @b{Spec}s.  Each bias weight
and each group of receiving weights (i.e., all the weights coming to a
particular unit from a particular layer) has associated with it a
pointer to a specification, and parameters such as the learning rate are
read from the specification when they are used.  Our network has been
set up so that all of the weights and biases share the same
specification.  This specification is called a connection specification
or @b{ConSpec}, and since it is the one we are using in this XOR network
we have called it "XORConSpec".

One can look at this specification by selecting in the @b{Project}
window @i{.specs} / @i{Edit} / @i{XORConSpec}.  When this pops up we can
see that there are actually several variables associated with this Spec,
including @code{lrate}, @code{momentum}, and @code{decay}.  We can
change any one of these, but for now click in the @code{lrate} field and
change the value to, say, 1.0.  Then click @i{Apply} or @i{Ok} (which
applies and closes the pop-up).

To see what happens with this higher learning rate, go ahead and @i{Re
Init} on the Train Control panel as before.  Before you hit @i{Run} you
may wish to @i{Clear} the @b{GraphLogView}.

One can modify the learning process in other ways. For example, one can
change the @code{momentum} parameter, or introduce @code{decay}.  These
parameters are also associated with the @b{ConSpec} and can be
modified in the same way as the @code{lrate}.  For more information
about these parameters, see @ref{bp-con}.

Another parameter of the Backpropagation model is the range of values
allowed when initializing the connection weights.  This is controlled by
the @code{var} parameter of the @code{rnd} field of the @b{ConSpec}.
The default value of 0.5 can be increased to larger values, such as 1.0,
thereby giving a larger range of variation in the initial values of the
weights (and biases).

One can also modify the learning process by selecting different options
in the @b{EpochProcess}.  For example, as initially configured, the XOR
problem is run using @code{SEQUENTIAL} order of pattern presentation,
and the mode of weight updating is @code{BATCH}.  @code{SEQUENTIAL}
order simply means that the patterns are presented in the order listed
in the environment.  The other two options are @code{PERMUTED} (each
pattern presented once per epoch, but the order is randomly rearranged)
and @code{RANDOM} (on each trial one of the patterns is selected at
random from the set of patterns, so that patterns only occur once per
epoch on the average).

The options for weight updating are @code{BATCH}, which means that the
weights are updated at the end of the epoch; @code{ON LINE}, which means
that the weights are updated after the presentation of each pattern,
@code{SMALL BATCH}, which means that the weights are updated after every
@code{batch_n} patterns, and @code{TEST}, which means that the weights
are not updated at all.

These characteristics of the processing are managed by the "Epoch_0"
process that is subordinate to the "Train_0" process---
and so to change them select @i{.processes} / @i{Edit} /
@i{Epoch_0} from the @b{Project} window menu bar.  The dialog that
pops up indicates the current values of these options (sequential and
batch) next to their labels (@code{order} and @code{wt update}).  Click
on the current value of the option and the alternatives will be
displayed; then select the one you want, and click @i{Apply} or @i{Ok}
when done.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-using-saving,  , tut-using-changing, tut-using
@subsection Saving, restoring, and exiting.

You can save the state of your simulation or virtually any part of it.
This is done by selecting @i{Object} / @i{Save} or @i{Object} / @i{Save
As} on the appropriate menu.  For example, to save the whole project
using its existing project name, you would select @i{Object} / @i{Save}
from the @b{Project} window.  To save just the network in the file
"myxor.net" you would select @i{Object} / @i{Save As} from the
@b{NetView} window and then enter the filename "myxor" in the filename
field of the popup before clicking @i{Ok}.  The program will actually
save the file as "myxor.net.gz"; the ".gz" suffix indicates that the
file has been compressed using @file{gzip}.

To restore the state of the simulation or some part of it, you can just
select @i{Object} / @i{Load} in the appropriate window.  This will load
the saved copy of the object over whatever is presently there,
effectively destroying whatever was there before.  When you first start
up the simulator, you can load a project by selecting @i{.project} /
@i{Open In} / @i{Root}.  This creates a project as a constituent of root
and loads the project saved in the file.

At the end of the day, once you've done whatever saving you want to do,
you can exit from the simulator by selecting @i{Object} / @i{Quit} from
the Root window.

Now you know how to run the software and you are done with
this part of the tutorial.  The next section of the tutorial steps
through the process of actually building a simulation from scratch
instead of loading a "canned" one in.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config,  , tut-using, tut
@section Configuring the Encoder Problem

Running a canned exercise may be fun and informative, but most people
like to use simulation models to work on something new of their own.  In
this short section we will give you a quick overview of the essentials
of creating your own project.  We'll use the 4-2-4 encoder problem,
first considered by Ackley, Hinton and Sejnowski in their seminal
article on learning in Boltzmann machines.  We'll stay with the back
propagation learning algorithm, though, since it learns the problem much
faster than the Boltzmann machine.

In the 4-2-4 encoder problem, the goal is to learn to associate an input
pattern with itself via a small hidden layer (in this case containing 2
units).  There are 4 input units and 4 output units, and there are 4
training pattern-pairs, each involving one unit on, the same unit, in
both the input and the output:

@example
Input   Output
1000 -> 1000
0100 -> 0100
0010 -> 0010
0001 -> 0001
@end example

What we need to do is set up a network with four input units, four
output units, and two hidden units; then create an environment
containing the set of training patterns; then create the processes that
train and test the network, then create Logs and LogViews for displaying
the results of training and testing.

To begin, you just start up the executable for bp by typing @kbd{bp++}
at your unix prompt.  This starts the program, giving you the
@code{bp++} prompt as your command-line interface to the program, and
giving you a Root window as the seed of the GUI you are about to build
for yourself.  You're going to do the whole thing through the GUI.  You
can keep a script of what we've done so you can repeat the same steps
(or even edit them and do something slightly different) later.  This is
optional, however; many people prefer simply to save the state of their
configured project in a .proj file.

To create your project, select @i{.projects} / @i{New} / @i{Project} in
the Root window, and then just click @i{Ok} on the confirmation box that
appears. A project menu will appear.

@menu
* tut-config-scripts::          Recording a Script File of Your Actions
* tut-config-networks::         Making a New Network
* tut-config-environments::     Making a New Environment
* tut-config-processes::        Setting Up Control Processes
* tut-config-logs::             Creating Logs For Viewing Data
* tut-config-misc::             A Few Miscellaneous Things
* tut-config-running::          Training and Testing Your Network 
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-scripts, tut-config-networks, tut-config, tut-config
@subsection Recording a Script File of Your Actions

If you would like to make a script file of your actions, you should
start it up before you start building the network.  The script will
record all your actions, which means you can later look at this script
and see what steps you took (and replay them).  However, because one
inevitably makes mistakes along the way, using a script to save
everything is not the best idea --- instead, we simply save a project
file at the end with everything stored as it is currently configured.

To start recording your script, select @i{.scripts} / @i{New} /
@i{Script} , and @i{Ok} the creation with the @emph{right} mouse button
(this tells the program you want to edit the object that you create).
An edit dialog window for the script object should automatically
appear. If you forgot to use the right mouse button, you can manually
bring up an edit dialog by choosing @i{.scripts} / @i{Edit} /
@i{Script_0}. You will want to associate a file with this script, so
click on the menu next to @code{script file} (where it says @code{--- No
File ---}) and select @i{Open}. A File Selection Window will appear with
an empty edit field near at the top under the text "Enter Filename:".
Fill in the name you want to give your script ("my424" would do) and
press the "Open" button at the bottom of the File Selection Window or
press <Enter>.  Once this is done you still have to click @i{Apply} back
on the script dialog to confirm your file name.  Now hit @i{Record} and
your future actions will be recorded in the script.  The mouse pointer
should change to an arrrow with the letters "REC" beneath it to indicate
that you are in record mode. To stop recording you can press the
@i{StopRec} button in this window. Since we do not need this window
until later you can iconify it now.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-networks, tut-config-environments, tut-config-scripts, tut-config
@subsection Making a New Network

Now, lets make the @b{Network}.  Select @i{.networks} / @i{New} /
@i{Network} and @i{Ok}.  You now have a blank @b{Network} object, and a
@b{NetView} window in which to view it.

Click @i{New Layer(s)}, set the @code{Number} to 3 (by editing the number
field or using the increment/decrement buttons), and hit @i{Ok}.  Your
three layers will appear, but at this point none of them contain any
units.  The @b{NetView} is now in @i{ReShape} mode (the corresponding
mode button is highlighted).  So, if you select "Layer_0", you can start
to reshape it.  Click and hold in the line you see above the layer label
(this line is a layer-frame with no space in it yet) and drag the mouse
to the right; boxes will appear corresponding to unit-spaces.  When you
have 4, stop and let go.  You've created a frame for four units.  Click
on the line above "Layer_1" to create a frame for the two hidden units;
drag right until two boxes appear.  Finally click on the line above the
"Layer_2" label, and create a frame for the four output units. You now
have frames but no units; to create the units, in all three layers at
once, just click on @i{Build All}.  The units should all now be visible,
and the default @i{act} variable selected, indicating that your are
viewing their activation states, which should all be zero.

You can move the hidden layer to be centered, by selecting @i{Move} mode
then dragging the hidden layer with the mouse.  Note that things move in
"chunks" of the size of a single unit, so you will have to move the
mouse a certain amount before the layer itself moves.  It's a good idea
to go back to Select mode after moving by hitting @i{Select}.

Also, clicking the @i{Init} button will adjust the display of the
network to fill the available space, and generally fix any display
problems.  It is automatically performed most of the time, but you can
use the @i{Init} button if the display doesn't look right for some
reason.

Now we need to make connections.  Actually, you first make
@b{Projection}s then create connections within the projections.  A
projection is to a connection what a layer is to a unit; it represents a
bunch of connections that connect units in two layers.  In any case, you
need to create two projections.  One to the hidden layer from the input
layer, and one to the output layer from the hidden layer.  In each case:

@itemize @bullet
@item 
Select the receiving layer with the left mouse button.  You want to
see the frame around all of the units highlighted.  This should occur on
the first click; if that's not what you see, keep clicking until it is; 
(you're cycling the selection through the layer, all the units, and the
unit under the mouse).  

@item
Add the sending layer to the selection using the "extend-select", which
can be done with either the middle mouse button, or the Shift key plus
the left mouse button, depending on whether you come from a Unix or a
Mac background.  Now both the sending and receiving layers should be
highlighted. If you make a mistake, click the left mouse button in the
background area of the NetView between the layers to unselect the layers
and start over with the previous step.

@item
When you have 2 layers selected, click @i{New Prjn(s)}, which will be
highlighted. A Projection arrow should appear between the two layers.
Note that the arrow head of the projection arrow is unfilled (outline)
indicating the the actual connections have not been created yet.
@end itemize

After you do this for both projections you will have two unfilled
arrows.  Now click @i{Connect All} and your projections will be filled
with connections to each unit on the receiving end from each unit on the
sending end. Now the arrow heads of the projections will be solid
indicating that their connections have indeed been created. You may
verify this by clicking on the @i{r.wt} button and using the finger
pointer to view the weights of some of the hidden and output units.
When you are finished looking around, return to viewing the activations
by pressing @i{act}.

And your network is complete.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-environments, tut-config-processes, tut-config-networks, tut-config
@subsection Making a New Environment

Now we can create the @b{Environment} and the training patterns.  Locate
the Project window (which you'll note has been updated to reflect the
existence of your new Network object), and select @i{.environments} /
@i{New} / @i{Environment} and @i{Ok}.  Your @b{EnviroView} window will
appear.  Then click @i{New Event}.  Set the number to create to 4, then
click @i{Ok}.  You should see Panel buttons for all four events
("Event_0".."Event_3").  Left-click the first one to select it for
viewing, then extend-select (middle-click or Shift + left-click) the
rest in order, and all of them will appear to the right of the buttons.
They automatically have an input pattern the same size as the input
layer and an output or target pattern the same size as the output layer,
and all the values of all of the patterns are initialized to 0.  Both
input and output patterns are displayed with the top four boxes of each
event represent the output target values and the lower four boxes
representing the input values. You can now set the 1 bits of each
pattern by clicking on the color spot next to the value field labeled
'1', then clicking on the appropriate elements of the pattern.  They
will turn to the appropriate color, indicating the specified value.  You
have to click @i{Apply} for the change to take effect.  Now you have
your environment.

You can also use this @b{EnviroView} window to configure the layout of
the patterns within the events according to the @b{EventSpec} and
@b{PatternSpec} specs.  The default layout (which works for our
present purposes) is to have one pattern for the first layer in the
network, which serves as an input, and another pattern for the last
layer in the network, which serves as an output.  However, these
defaults will not always be appropriate.  To see how to change them, hit
the @i{Edit Specs} button at the top left of the window, and then after
the display changes, hit the @i{EventSpec_0} button.  You will see two
grids for the two patterns (input and output).  You can move, reshape,
and edit these patterns if you need to.  The text within each pattern
shows some of the critical settings in terms of whether an pattern is an
@code{INPUT} or @code{TARGET}, what layer it goes to, etc.  We'll just
leave everything as-is for now, but if for example you wanted to change
the network to be a 8-3-8 encoder, then you'd need to reshape these
patterns to be 8 units wide instead of 4.  Doing so would automatically
stretch the corresponding events that use this spec.

For now, just hit @i{Edit Events} to return to the events display, and
iconify the window.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-processes, tut-config-logs, tut-config-environments, tut-config
@subsection Setting Up Control Processes

While the @b{Network} and the @b{Environment} constitute the bulk of the
custom aspects of a simulation, the way in which the network is trained
can vary depending on what kinds of tasks you are performing, etc.  The
training and testing of a network is determined by the processes, and
the results of running these processes can be viewed in logs that record
data over time.

Processes come in hierarchies, but there are sensible defaults built
into the program that create sensible hierarchies. We will create two
hierarchies, one for training the network, and one for testing.  Should
your process hierarchy become accidentally misaligned with unnecessary
Train, Epoch and Testing processes or spurious Batch or Sequence
processes, you may wish to start the process creation procedure over
from scratch by choosing @i{.processes} / @i{Remove} / @i{All}.
In this case, the default naming of the processes may use a different
numbering convention than the one described below, but hopefully you
will be able to follow along.

To create the @b{TrainProcess}, select @i{.processes} / @i{New} /
@i{TrainProcess} from the Project window.  A New Object Popup window
will appear indicating that 1 Train Process should be created. In
addition, the @i{Create Subprocs} toggle will be checked; leave this, it
will do the right thing and create a subordinate Epoch and Trial process
under the Train process for you automatically.  Use the right mouse
button to click @i{Ok} so you can edit the Train process.  If you forget
to use the right mouse button, you can edit the train process by
selecting @i{.processes} / @i{Edit} / @i{Train_0}. You could at this
point rename the train process to something more descriptive, (e.g.,
"Training") by changing its name field, but we'll assume you stay with
the default, which is just Train_N, where N is the index of this
instance of a train process; if this is the first process you are
creating, the process will be called Train_0.  One thing you can notice
here is the max epoch value associated with the Train process.  By
default it has value 1000, which is reasonable for many relatively small
scale problems --- if it doesn't learn in 1000 epochs, it may not learn
at all.  We are done with editing the Train Process at this point, so
you should press @i{Ok} and dismiss the Edit Dialog.

Now, take a look at your Project window, which shows all of the major
objects within your project.  You should see the network and
environment, and the three processes in the process hierarchy you just
created.  You can automatically view or iconify the network or
environment windows by double-clicking on their icons.  Also, you can
see how everything is connected up through the processes by clicking on
a process and hitting show links --- select the yellow @i{Trial_0} icon
and then hit the @i{Show Links} button.  You should see that this
process works on the network (solid pink line) and the environment
(solid green line), and that it updates the network display (dashed pink
line).  You can also see the statistics that are computed at each level
of processing (more on this below).

We can now edit the sub-process Epoch_0, either clicking with the right
mouse button on the @i{Epoch_0} object in the project view, or by
choosing its name from the @i{.processes} / @i{Edit} / @i{name}
menu. Again, you could rename them at this point; but we'll assume you
stay with the default name of Epoch_0.  The main reason to edit the
Epoch process is to specify the presentation order and what mode of
weight updating you want by setting the values of the @i{order} and
@i{wt update} fields in the edit dialog for Epoch_0.  The defaults,
@code{PERMUTED} and @code{ONLINE} may not be what you want
(@code{SEQUENTIAL} @code{BATCH} is the "standard" combination).

By default, the TrialProcess for the Bp algorithm creates a
squared-error statistic.  You can see this statistic in the project
view, or by pulling down the edit menu, and moving the mouse over any of
the processes --- it shows up in a sub-menu off of the these processes.
In general, statistics are created at the @b{TrialProcess} level,
because that is the appropriate time-grain for the computation of most
statistics.  For the squared-error, this is when an individual pattern
has been presented, the activations propagated through the network, and
a difference between the target and actual output values is available
for computing the error.  However, one often wants to see an
@i{aggregation} of the error (and other statistic) over higher levels of
processing (e.g., a sum of the squared-error over an entire epoch of
training).  This is done by creating aggregates of the statistic, and it
is why all of the processes contain a squared-error statistic in them --
the ones in the Epoch and Train processes are aggregations of the one
computed in the TrialProcess.

While you could procede to the next step now, we will make a small
detour in order to show you how to create statistics, since you may in
the future want to create a different type of statistic than the default
squared-error statistic.  We will simply remove the existing statistic,
and then perform the steps necessary to recreate it.  To remove the
statistic, you can click on it (under the @i{Trial_0} process) and hit
@i{Rmv Obj(s)} button in the project view, or select @i{.processes} /
@i{Remove} / @i{Trial_0} / @i{sum_Trial_0_SE_Stat}, and confirm that it
is @i{Ok} to close (remove) this item.  Note that the SE_Stat has been
removed from all of the processes --- a statistic will always remove its
aggregators in higher-level processes when it is removed.

Now we will perform the steps necessary to re-create the statistic we
just removed.  You can do this by either selecting the @i{Trial_0}
object in the project view, and hitting @i{New Stat}, or by selecting
@i{.processes} / @i{New Stat} / @i{SE_Stat} in the @b{Project} window
(@i{New Stat} is near the bottom of the menu).  The popup that appears
indicates the type and number; it also indicates which process the stat
will be created in.  As mentioned above, statistics are generally
created at the @b{TrialProcess} level, because that is the appropriate
time-grain for the computation of most statistics.  Given that there is
only one Trial process (Trial_0) at this point, the program will guess
correctly and suggest to create the SE_Stat at this level.  The popup
also allows you to choose where within the Trial process to create the
statistic -- with the two options being Loop and Final.  Most statistics
will know where to create themselves, so the @i{DEFAULT} selection
should be used, which means it is up to the stat's own default where to
go.  You'll want to create aggregates (summing over trials at the epoch
level, and keeping track of the latest value at the overall Train
process level), so leave the @i{Create Aggregates} toggle checked.
Click @i{Ok}.

Now you get to answer another question: What aggregation operator do you
want for the next (epoch) level?  The default @code{LAST} (keep track of
just the last value) is not what we need; you want to @code{SUM} this
stat at the next level, so select @code{SUM}, and then click @i{Ok}.

Note that the stat is also being aggregated at the levels above the
epoch (i.e. the @b{Train} level).  It will keep track of the last value
at the @b{Train} level.  Note that at each level each statistic has a
name refelcting the process and the nature of the aggregation.  So the
Train level stat is called "lst_sum_Train_0_SE_Stat" indicating it is
the last value of the sum of an @b{SE_Stat} and that it is attached to
the Train level.  We have now re-created the squared-error statistic.
You could have skipped over this process, but now you know how to do
this in the future.

Finally, you need to set the stopping criterion for learning, which is
not set by default.  You can do this by editing the "Train_0_SE_Stat".
You find it by right-clicking (or selecting and hitting @i{Edit}) the
object in the project view, or by doing @i{.processes} / @i{Edit} /
@i{Train_0} / @i{lst_sum_Train_0_SE_Stat}.  Once this pops up you will
find the @code{se} value of this stat near the bottom of the
window. This field has 1 parameter @code{val} which indicates the
current value of the statistic, and 4 parameters which control when the
stopping actually occurs. To set the criteria you need to click on the
first stopping paramater, @code{stopcrit} which should turn on with a
checkmark symbol indicating that we are indeed using this statistic as a
stopping criteria. The default relation @code{<=} on the next parameter
to the right is appropriate here so ther is no need to adjust its
value. To the right of the relation parameter is the value to which the
statistic is measured against. In our case, the @code{val} of this SE
statistic is compared with this value to determine if it is less than or
equal to it.  A reasonable value for this parameter is .04 (.01 for each
pattern you are testing, summed over patterns). The next field labeled
@code{cnt} can be used to set a stricter stopping criterion where the
condition must be met @code{cnt} times in a row instead of just once.
We will just leave the value at 1. Click @i{Ok} when you are done.

You will probably also want to create a Test process hierarchy.  A test
sweeping through all of your patterns is an @b{EpochProcess}.  So,
select @i{.processes} / @i{New} / @i{EpochProcess}. After you click
@i{OK} (use right mouse button to edit) in the popup window, go ahead
and edit this process (if you didn't use right-click, select
@i{.processes} / @i{Edit} / @i{Epoch_1}).  Set @code{wt_update} to
@code{TEST} and @code{order} to @code{SEQUENTIAL}.  Click @i{Ok}.  Note
that a default SE_Stat was automatically created for your test process.
There is no need to set any stopping criteria.

Now, you probably want to monitor some aspect of network function in
your test.  Let's look at the activation of the output units.  To do
this:

@itemize @bullet
@item
Select the units in the output layer in the @b{NetView} (either the whole
layer, or all the units in the layer).

@item
Select @i{act} to display on the @b{NetView}.

@item
Click @i{Monitor Values} / @i{New} (located in the left-middle of the
@b{NetView} display).

@item
In the pop-up, set the process to "Trial_1".  You'll find it under
@i{BpTrial} when you click on the @code{In Process} value field.
Everything else is as we want it so click @i{Ok}.
@end itemize

Also, it is useful to create an EpochCounterStat for the test process by
selecting @i{.processes} / @i{NewStat} / @i{EpochCounterStat}, then,
when the pop-up appears, for @i{In Process}, select @i{Trial_1}, then
click @i{OK} (or select @i{loop_stats} under the @i{Trial_1} object in
the project view, and then pick @i{EpochCounterStat} from the dialog).
When the program asks you how to aggregate this statistic, it will
remember the value you entered last time (@code{SUM}), but we will want
to use @code{LAST} this time. This will allow you to record information
about how many epochs the network has been trained on each time you run
a test.

Now, so you'll be able to run the net and test it, you can create
control panels for training and testing.  In the project view, select
the @i{Train_0} object and hit @i{Ctrl Panel}, and then the same for
@i{Epoch_1}, or do @i{.processes} / @i{Control Panel} / @i{Train_0} for
the training panel and @i{...} / @i{Epoch_1} for the testing panel.  In
the Train_0 control panel, you might set the @code{step} @code{n} field
to something like 30, so that when you click step it will run for 30
epochs.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-logs, tut-config-misc, tut-config-processes, tut-config
@subsection Creating Logs For Viewing Data

Before you run the network, you need to create logs of the results of
training and testing and views that allow you to examine the contents of
these logs.  We'll create a Log with a @b{GraphLogView} to follow the
@b{SE_Stat} over epochs and Log with a @b{GridLogView} to follow the
activations of the output units at test.

To create the @b{GraphLogView}, select @i{.logs} / @i{New} /
@i{GraphLog}, and hit @i{Ok} in the New dialog.  It will automatically
prompt you for which process you want to update this view, which should
be @i{Epoch_0}.  This will cause the @b{GraphLogView} to get information
from this process about what is being monitored there: the @b{SE_Stat},
summed across output units and across patterns in the epoch.  By default
the epoch number will be used at the X axis, so this GraphLogView is now
ready.

To create the @b{GridLogView}, select @i{.logs} / @i{New} / @i{GridLog},
and specify that @i{Trial_1} should update this log.  Again, the header
for the data that are being monitored is automatically retrieved.

As an advanced example of setting up a grid log, we now describe how you
may be able to set up a special GridLog associated with your Test process
(Epoch_1) that displays the weights in your network at the end of each
test that you run.  This may or may not be something you really need to
do depending on your goals.  But the example shows some advanced
features that are useful and powerful, so we go through it to expose you
to them.

1a. Create a monitor stat for each projection.  Make sure @i{r.wt} is the
current variable displayed, and that you are in @code{Select} mode
instead of @code{View} mode. Now select the projection in the netview
which connects the hidden and output layers. Then click @i{Monitor
Values} / @i{new}.  For @i{In Process}, select @i{EpochProcess} /
@i{Epoch_1}, and for @i{Loop/Final} select @i{FINAL}, then @i{OK}. We
selected @i{Final} here since we want to log the values of the weight at
the end of each epoch.

1b. Repeat step 1 selecting the input->hidden layer projection instead.

2. Create a new GridLog with @i{.logs} / @i{New} / @i{GridLog}, then
@i{OK} the popup, and select @i{Epoch_1} as the updater for this log.

3. Now comes the interesting part.  We're going to re-arrange this
display so it displays the weights in a way that better reflects the
network's structure.  To do this we are going to change the geometry and
layout of the weight matrices for each projection.  In the GridLog there
is a "header" at the top associated with each column in the log (the
sum-squared error, the EpochCounterStat (labeled @i{epoch}) and each of
the two projections you are monitoring, each labeled @i{wt}).  We can
manipulate these headers with the mouse to rearrange their layout.

3a. Use the middle button (or shift plus left button) to reshape the
layout.  We first want to reshape the hidden-to-output weights to be 2
units wide by 4 units tall, so that each row of units will be the
weights from one of the four output units.  Move the mouse to the right
hand side of the first @i{wt} grid, and middle-click and hold it down
while dragging upwards and to the left into a shape that is 4 tall and 2
wide --- it will not let you configure a shape that doesn't hold all 8
of the weight values, so you need to keep that in mind as you configure.
It may take a few tries to get this right.

3b. Next we want to reshape the input-to-hidden weights to be 4 units
wide by 2 tall, so that gain one row represents the weights for one
receiving unit.   Do this using the right mouse button.  You can also
use the left mouse button to move the columns around, and if you want to
relabel the columns, that can be done by right-mouse-button clicking on
the headers.

4. Then, press @i{Run} in the test epoch process control panel to see
the weights!  To verify the weigh values and understand how they
correspond to those in the NetView, click on @i{r.wt} in the NetView,
(make sure you are in @i{View} mode), and select the first hidden unit.
The weights from the input units should be the same as those in the
GridLog for the first row of the bottom set of weights.  Similarly, the
second hidden unit's weights are those in the second row of the bottom
set of weights.  The next set of weights are best viewed using @i{s.wt}
to look at the @emph{sending} weights from the hidden units to the
output units.  The sending weights for the first hidden unit are shown
in the top row of the top set of weights in the GridLog.  Those for the
second hidden unit are in the second row.  This will all be much clearer
in a fully trained network!

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-misc, tut-config-running, tut-config-logs, tut-config
@subsection A Few Miscellaneous Things

Before you actually run the project, we'll mention a couple of final
things you will want to do.

You can watch the state of the network in the @b{NetView} while it
learns and during testing.  The NetView is automatically updated by the
Trial processes in both the training and testing process hierarchies.
(You can have it updated by other processes by selecting @i{View:Object}
/ @i{Add Updater} / @i{<process>} in the NetView, or by clicking on a
process and a network in the project view, and hitting @i{Add Updater}).
Both of these Trial processes will send the @b{NetView} an update
signal, so you can see the values of whatever state variable you'd like
to look at updated after each trial of training or testing.  You might
select @code{act} as the variable to display in the NetView, toggle
@code{Auto Range} off, and set the @code{max} and @code{min} on the
color scale to 1 and -1.  That way the meanings of the color in the
color scale stay fixed as the various patterns are presented.

If you are recording the project using a script, you might want to turn
off the scripting process at this point, since the network creation
process is complete.  De-iconify the script and click @i{StopRec}.  You
may @i{Edit} your script (which should pull up an editor and allow you
to view the script file), to see what steps were taken, etc.

Regardless of whether you recorded a script, you will want to save the
state of the project as it is now, so that it can simply be re-loaded
from a project file, just like the XOR example.  Select @i{Object} /
@i{Save As} in the @i{Project} window, and specify a file name for
saving (a ".proj.gz" will automatically be added to the end of the file
name). 

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  tut-config-running,  , tut-config-misc, tut-config
@subsection Training and Testing Your Network

Finally, you are ready to run your project.

Now you can run through a Test --- just hit @i{Run} in the Epoch_1
control panel.  You will see the four patterns flicker through the
@b{NetView}, and you will see them logged in the @b{GridLogView}.  The
outputs all look pretty much the same, at this point.  But now you can
start to train the network.  Click on @i{Step} or @i{Run} in the Train
control panel, and you are off and running.

You can play around with parameters, etc. as described in the previous
tutorial on running XOR.

There is one very important aspect of PDP++ which has not yet been
demonstrated in the tutorial, which we can step through at this point.
This is the ability to apply different parameters to different parts of
the network.  For the purposes of demonstration, we will assume that we
want to have one learning rate for the input-to-hidden weights, and
another learning rate for the hidden-to-output weights.  The basic idea
about how to do this is to create two different @b{ConSpec} objects, one
of which applies to the input-to-hidden weights, and another of which
applies to the hidden-to-output weights.  An important facility in PDP++
is the ability to create a @emph{child} spec that automatically inherits
a set of parameters from a @emph{parent} spec, and has a set of unique
parameters.  Thus, for the present example, we would want to create a
child ConSpec that inherits everything from the parent except for the
learning rate, which will be unique to it.  Thus, we will have two
conspecs that are identical except for the learning rate.   The
advantage of this setup is that if you should decide to manipulate other
parameters such as momentum, weight decay, etc, the two specs will
automatically get the same values of these changed parameters.

To create the new child ConSpec, locate the Project window, and do
@i{.specs} / @i{New Child} / @i{BpConSpec_0} (note that @i{New Child} is
at the bottom of the menu).  This will bring up a New object dialog,
specifying that the new BpConSpec will be created as a child of the
existing one.  Select @i{Ok} with the right mouse button, so that the
new child will be edited.  Then, click the mouse into the @i{lrate}
field, and enter a new learning rate parameter (e.g., .01).  Notice that
the little check-box next to this field was checked when we clicked in
lrate.  This indicates that this is a @emph{unique} parameter, while the
other, non-checked boxes are @emph{inherited} parameters.  To see how
this works, let's edit the parent spec.  At the top of the edit dialog,
select @i{Actions} / @i{Find Parent}, which will bring up the edit
dialog for the parent con spec.  Notice that the parent does not have
any of the unique check boxes, since it does not inherit from anything
else.  Now, change another parameter, like @i{momentum}, in the
parent dialog, and press @i{Apply}.  The @i{Revert} button on the child
spec is highlighted, so press it.  Notice that the child dialog displays
the new (changed) momentum value, but retains the unique learning rate
parameter entered before.  Elaborate hierarchies of specs can be
created, and the patterns of inheritence and unique parameters provides
a very clear indication what is specialized about specific spec relative
to the others, and what it has in common.

Before closing the edit dialogs, it is a good idea to label them with
mnemonic names -- call the parent "input-to-hidden cons" and the child
"hidden-to-output cons", for example.  Then click @i{Ok} on both edit
dialogs.

Having created the specs with different learning rates, we now need to
specify that one spec applies to one set of connections, and another
applies to the other set.  As it is now, the parent spec applies to all
projections, since it is the default that was created automatically when
the connections were created.  The best way to set specs is using the
NetView, which has a @i{Selections} menu that operates on selected items
in the NetView (it is on the view half (right side) of the window, since
it operates on the items selected in a particular view).  Thus, make
sure you are in @i{Select} mode, and select the projection arrow that
goes from the hidden to the output layer.  Then, select @i{Selections} /
@i{Set Con Spec}.  This will bring up a popup dialog, where you should
select @i{BpConSpec} / @i{hidden-to-output cons}, and click @i{Ok}.
This has now set the selected projection to use this con spec instead of
the default.  To verify the status of the network, click in the
backround of the NetView to unselect everything, and then choose
@i{Selections} / @i{Show Con Spec}.  In the dialog, select
@i{hidden-to-output cons}, and click @i{Ok}.  The correct projection
arrow will be selected in the NetView, indicating that it is using that
con spec.  You can repeat this to verify that the other projection is
still using "input-to-hidden cons".  Thus, the @i{Selections} menu
allows you to both set and verify which objects in the network are using
which specs.  Notice that you can set many other properties using this
menu much in the same way we just did for ConSpecs on projections.

At this point, you might want to look at some of the other demo projects
available.  These are located in the @file{demo} directory where the XOR
example project was.  Check out the @file{README} files for further
information.


