@c uncomment the following two lines for 'update every node' command
@c @node  so
@c @chapter Self-organizing Learning

@cindex Self-organizing Learning
@cindex Learning, Self-organizing
@cindex Associative Learning
@cindex Hebbian Learning
@cindex Competitive Learning
@cindex Self-organizing Map

The defining property of self-organizing learning is that it operates
without requiring an explicit training signal from the environment.
This can be contrasted with error backpropagation, which requires target
patterns to compare against the output states in order to generate an
error signal.  Thus, many people regard self-organizing learning as more
biologically or psychologically plausible, since it is often difficult
to imagine where the explicit training signals necessary for
error-driven learning come from.  Further, there is some evidence that
neurons actually use something like a Hebbian learning rule, which is
commonly used in self-organizing learning algorithms.

There are many different flavors of self-organizing learning.  Indeed,
one of the main differences between self-organizing algorithms and
error-driven learning is that they need to make more assumptions about
what good representations should be like, since they do not have
explicit error signals telling them what to do.  Thus, different
self-organizing learning algorithms make different assumptions about the
environment and how best to represent it. 

One assumption that is common to many self-organizing learning
algorithms is that events in the environment can be @emph{clustered}
together according to their "similarity."  Thus, learning amounts to
trying to find the right cluster in which to represent a given event.
this is often done by enforcing a competition between a set of units,
each of which represents a different cluster.  The @emph{competitive
learning} algorithm (CL) of @cite{Rumelhart and Zipser, 1985} is a
classic example of this form of learning, where the single unit which is
most activated by the current input is chosen as the "winner" and
therefore gets to adapt its weights in response to this input pattern.

The PDP++ implementation of self-organizing learning, called @emph{So},
includes competitive learning and several variations of it, including
"soft" competitive learning @cite{Nowlan, 1990}, which replaces the
"hard" competition of standard competitive learning with a more graded
activation function.  Also included are a couple of different types of
modified Hebbian learning rules that can be used with either hard or
soft activation functions.

An additional assumption that can be made about the environment is that
there is some kind of @emph{topology} or ordered relationship among the
different clusters.  This notion is captured in the
@emph{self-organizing map} (SOM) algorithm of @cite{Kohonen, 1989; 1990;
1995}.  This algorithm adds to the basic idea of competition among the
units that represent a cluster the additional assumption that units
which are nearby in 2-D space should represent clusters that are somehow
related.  This spatial-relatedness constraint is imposed by allowing
nearby units to learn a little bit when one of their neighbors wins the
competition.  This algorithm is also implemented in the So package.

The directory @file{demo/so} contains two projects which demonstrate the
use of both the competitive-learning style algorithms, and the
self-organizing maps.

@menu
* so-over::                     Overview of the So Implementation
* so-con::                      So Connection Specifications
* so-unit::                     So Unit and Layer Specifications
* so-proc::                     The So Trial Process
* so-impl::                     So Implementational Details
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  so-over, so-con, so, so
@section Overview of the So Implementation

The So implementation is designed to be used in a mix-and-match fashion.
Thus, there are a number of different learning algorithms, and several
different activation functions, each of which can be used with the
other.  The learning algorithms are implemented as different connection
specs derived from a basic @b{SoConSpec} type.  They all use the same
@b{SoCon} connection type object.

Unlike the other algorithms in the PDP++ distribution (Bp and Cs), the
So implementation uses @b{LayerSpec} objects extensively.  These layer
specifications implement competition among units in the same layer,
which is central to the self-organizing algorithms.  Thus, there are
three different layer specs all derived from a common @b{SoLayerSpec}
which implement hard competitive learning (@b{Cl}), soft competitive
learning (@b{SoftCl}), and the self-organizing map (@b{Som}) activation
functions.

There are no new statistics defined for self-organizing learning, and
only one process object, which performs a simple feed-forward processing
trial (all of the So algorithms are feed-forward).

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  so-con, so-unit, so-over, so
@section So Connection Specifications
@cindex Connections, So
@cindex Self-organizing Connections
@tindex SoCon
@tindex SoConSpec
@tindex SoCon_Group

@vindex dwt of SoCon
@vindex pdw of SoCon
The basic connection type used in all the algorithms, @b{SoCon} has a
delta-weight variable @code{dwt} and a previous-delta-weight variable
@code{pdw}.  @code{dwt} is incremented by the current weight change
computations, and then cleared when the weights are updated.  @code{pdw}
should be used for viewing, since @code{dwt} of often zero.   While it
has not been implemented in the standard distribution, @code{pdw} could
be used for momentum-based updating (@pxref{bp-con}).

@vindex lrate of SoConSpec
@vindex wt_range of SoConSpec
@vindex avg_act_source of SoConSpec.
The basic @b{SoConSpec} has a learning rate parameter @code{lrate}, and
a range to keep the weight values in: @code{wt_range}.  Unlike
error-driven learning, many self-organizing learning algorithms require
the weights to be forcibly bounded, since the positive-feedback loop
phenomenon of associative learning can lead to infinite weight growth
otherwise.  Finally, there is a variable which determines how to compute
the average and summed activation of the input layer(s), which is needed
for some of the learning rules.  If the network is fully connected, then
one can set @code{avg_act_source} to compute from the
@code{LAYER_AVG_ACT}, which does not require any further computation.
However, if the units receive connections from only a sub-sample of the
input layer, then the layer average might not correspond to that which
is actually seen by individual units, so you might want to use
@code{COMPUTE_AVG_ACT}, even though it is more computationally
expensive.

The different varieties of @b{SoConSpec} are as follows:
@table @b
@item HebbConSpec
@tindex HebbConSpec
This computes the most basic Hebbian learning rule, which is just the
coproduct of the sending and receiving unit activations:
@example
  cn->dwt += ru->act * su->act;
@end example
Though it is perhaps the simplest and clearest associative learning
rule, its limitations are many, including the fact that the weights will
typically grow without bound.  Also, for any weight decrease to take
place, it is essential that activations be able to take on negative
values.  Keep this in mind when using this form of learning.  One
application of this con spec is for simple pattern association, where
both the input and output patterns are determined by the environment,
and learning occurs between these patterns.

@item ClConSpec
@tindex ClConSpec
This implements the standard competitive learning algorithm as described
in @cite{Rumelhart & Zipser, 1985}.  This rule can be seen as attempting
to align the weight vector of a given unit with the center of the
cluster of input activation vectors that the unit responds to.  Thus,
each learning trial simply moves the weights some amount towards the
input activations.  In standard competitive learning, the vector of
input activations is @emph{normalized} by dividing by the sum of the
input activations for the input layer, @code{sum_in_act} (see
@code{avg_act_source} above for details on how this is computed).
@example
  cn->dwt += ru->act * ((su->act / cg->sum_in_act) - cn->wt);
@end example
The amount of learning is "gated" by the receiving unit's activation,
which is determined by the competitive learning function.  In the
winner-take-all "hard" competition used in standard competitive
learning, this means that only the winning unit gets to learn.
Note that if you multiply through in the above equation, it is
equivalent to a Hebbian-like term minus something that looks like weight
decay:
@example
  cn->dwt += (ru->act * (su->act / cg->sum_in_act)) - (ru->act * cn->wt);
@end example
This solves both the weight bounding and the weight decrease problems
with pure Hebbian learning as implemented in the @b{HebbConSpec}
described above.

@item SoftClConSpec
@tindex SoftClConSpec
This implements the "soft" version of the competitive learning learning
rule @cite{Nowlan, 1990}.  This is essentially the same as the "hard"
version, except that it does not normalize the input activations.  Thus,
the weights move towards the center of the actual activation vector.
This can be thought of in terms of maximizing the value of a
multi-dimensional Gaussian function of the distance between the weight
vector and the activation vector, which is the form of the learning rule
used in soft competitive learning.  The smaller the distance between
the weight and activation vectors, the greater the activation value.
@example
  cn->dwt += ru->act * (su->act - cn->wt);
@end example
This is also the form of learning used in the self-organizing map
algorithm, which also seeks to minimize the distance between the weight
and activation vectors.  The receiving activation value again gates the
weight change.  In soft competitive learning, this activation is
determined by a soft competition among the units.  In the SOM, the
activation is a function of the activation kernel centered around the
unit with the smallest distance between the weight and activation
vectors.

@item ZshConSpec
@tindex ZshConSpec
@vindex soft_wt_bound on ZshConSpec
This implements the "zero-sum" Hebbian learning algorithm (ZSH)
@cite{O'Reilly & McClelland, 1992}, which implements a form of
@emph{subtractive} weight constraints, as opposed to the
@emph{multiplicative} constraints used in competitive learning.
Multiplicative constraints work to keep the weight vector from growing
without bound by maintaining the length of the weight vector normalized
to that of the activation vector.  This normalization preserves the
ratios of the relative correlations of the input units with the cluster
represented by a given unit.  In contrast, the subtractive weight
constraints in ZSH exaggerate the weights to those inputs which are
greater than the average input activation level, and diminish those to
inputs which are below average:
@example
  cn->dwt += ru->act * (su->act - cg->avg_in_act);
@end example
where @code{avg_in_act} is the average input activation level.  Thus,
those inputs which are above average have their weights increased, and
those which are below average have them decreased.  This causes the
weights to go into a corner of the hypercube of weight values (i.e.,
weights tend to be either 0 or 1).  Because weights are going towards
the extremes in ZSH, it is useful to introduce a "soft" weight bounding
which causes the weights to approach the bounds set by @code{wt_range}
in an exponential-approach fashion.  If the weight change is greater
than zero, then it is multiplied by @samp{wt_range.max - cn->wt}, and if
it is less than zero, it is multiplied by @samp{cn->wt - wt_range.min}.
This is selected by using the @code{soft_wt_bound} option.

@item MaxInConSpec
@tindex MaxInConSpec
This learning rule is basically just the combination of SoftCl and Zsh.
It turns out that both of these rules can be derived from an objective
function which seeks to maximize the input information a unit receives,
which is defined as the signal-to-noise ratio of the unit's response to
a given input signal @cite{O'Reilly, 1994}.  The formal derivation is
based on a different kind of activation function than those implemented
here, and it has a special term which weights the Zsh-like term
according to how well the signal is already being separated from the
noise.  Thus, this implementation is simpler, and it just combines Zsh
and SoftCl in an additive way:
@example
  cn->dwt += ru->act * (su->act - cg->avg_in_act) +
             k_scl * ru->act * (su->act - cn->wt);
@end example
Note that the parameter @code{k_scl} can be adjusted to control the
influence of the SoftCl term.  Also, the @code{soft_wt_bound} option
applies here as well.
@end table

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  so-unit, so-proc, so-con, so
@section So Unit and Layer Specifications
@cindex Units, Self-organizing
@cindex Layers, Self-organizing
@cindex Self-organizing Units
@cindex Self-organizing Layers
@tindex SoUnit
@tindex SoUnitSpec

Activity of units in the So implementation is determined jointly by the
unit specifications and the layer specifications.  The unit
specifications determine how each unit individually computes its net
input and activation, while the layer specifications determine the
actual activation of the unit based on a competition that occurs between
all the units in a layer.

@vindex act_i of SoUnit
All So algorithms use the same unit type, @b{SoUnit}.  The only thing
this type adds to the basic @b{Unit} is the @code{act_i} value, which
reflects the "independent" activation of the unit prior to any
modifications that the layer-level competition has on the final
activations.  This is primarily useful for the soft Cl units, which
actually transform the net input term with a Gaussian activation
function, the parameters of which can be tuned by viewing the resulting
@code{act_i} values that they produce.

There are three basic types of unit specifications, two of which derive
from a common @b{SoUnitSpec}.  The @b{SoUnitSpec} does a very simple
linear activation function of the net input to the unit.  It can be used
for standard competitive learning, or for Hebbian learning on linear
units.

@tindex SoftClUnitSpec
@vindex var of SoftClUnitSpec
The @b{SoftClUnitSpec} computes a Gaussian function of the distance
between the weight and activation vectors.  The variance of the Gaussian
is given by the @code{var} parameter, which is not adapting and shared by
all weights in the standard implementation, resulting in a fixed
spherical Gaussian function.  Note that the @code{net} variable on units
using this spec is the distance measure, not the standard dot product of
the weights and activations.

@tindex SomUnitSpec
The @b{SomUnitSpec} simply computes a sum-of-squares distance function
of the activations and weights, like the @b{SoftClUnitSpec}, but it does
not apply a Gaussian to this distance.  The winning unit in the SOM
formalism is the one with the weight vector closest to the current input
activation state, so this unit provides the appropriate information for
the layer specification to choose the winner.

There are three algorithm-specific types of layer specifications,
corresponding to the Cl, SoftCl, and SOM algorithms, and the parent
@b{SoLayerSpec} type which simply lets the units themselves determine
their own activity.  Thus, the @b{SoLayerSpec} can be used when one does
not want any competition imposed amongst the units in a layer.  This can
be useful in the case where both layers are clamped with external
inputs, and the task is to perform simple pattern association using
Hebbian learning.  Note that all layer specs do not impose a competition
when they are receiving external input from the environment.

@vindex netin_type of SoLayerSpec
There is one parameter on the @b{SoLayerSpec} which is used by the
different algorithms to determine how to pick the winning unit.  If
@code{netin_type} is set to @code{MAX_NETIN_WINS}, then the unit with
the maximum net input value wins.  This is appropriate if the net input
is a standard dot-product of the activations times the weights (i.e.,
for standard competitive learning).  If it is @code{MIN_NETIN_WINS},
then the unit with the minimal net input wins, which is appropriate when
this is a measure of the distance between the weights and the
activations, as in the SOM algorithm.  Note that soft competitive
learning does not explicitly select a winner, so this field does not
matter for that algorithm.

@tindex ClLayerSpec
The @b{ClLayerSpec} selects the winning unit (based on
@code{netin_type}), and assigns it an activation value of 1, and it
assigns all other units a value of 0.  Thus, only the winning unit gets
to learn about the current input pattern.  This is a "hard"
winner-take-all competition.

@tindex SoftClLayerSpec
@vindex softmax_gain of SoftClLayerSpec
The @b{SoftClLayerSpec} does not explicitly select a winning unit.
Instead, it assigns each unit an activation value based on a @emph{Soft
Max} function:

@iftex
@tex
% html a_j = e^g_j / (SUM_k e^g_k)
$$   a_j = {e^{g_j} \over {\sum_k e^{g_k}}} $$
@end tex
@end iftex
@ifinfo
     a_j = e^g_j / (SUM_k e^g_k)
@end ifinfo

Where @i{g_j} is the Gaussian function of the distance between the
unit's weights and activations (stored in @code{act_i} on the @b{SoUnit}
object).  Thus, the total activation in a layer is normalized to add up
to 1 by dividing through by the sum over the layer.  The exponential
function serves to magnify the differences between units.  There is an
additional @code{softmax_gain} parameter which multiplies the Gaussian
terms before they are put through the exponential function, which can be
used to sharpen the differences between units even further.

Note that @b{SoftClLayerSpec} can be used with units using the
@b{SoUnitSpec} to obtain a "plain" SoftMax function of the dot product
net input to a unit.

@tindex SomLayerSpec
@vindex neighborhood of SomLayerSpec
@vindex wrap of SomLayerSpec
Finally, the @b{SomLayerSpec} provides a means of generating a
"neighborhood kernel" of activation surrounding the winning unit in a
layer.  First, the unit whose weights are closest to the current input
pattern is selected (assuming the @b{SomUnitSpec} is being used, and the
@code{netin_type} is set to @code{MIN_NETIN_WINS}).  Then the neighbors
of this unit are activated according to the @code{neighborhood} kernel
defined on the spec.  The fact that neighboring units get partially
activated is what leads to the development of topological "map"
structure in the network.

The shape and weighting of the neighborhood kernel is defined by a list
of @b{NeighborEl} objects contained in the @code{neighborhood} member.
Each of these defines one element of the kernel in terms of the offset
in 2-D coordinates from the winning unit (@code{off}), and the
activation value for a unit in this position (@code{act_val}).
While these can be created by hand, it is easier to use one of the
following built-in functions on the @b{SomLayerSpec}:


@tindex SomLayerSpec
@table @code
@item KernelEllipse(int half_width, int half_height, int ctr_x, int ctr_y)
@findex KernelEllipse on SomLayerSpec
This makes a set of kernel elements in the shape of an ellipse with the
given dimensions and center (typically 0,0).
@item KernelRectangle(int width, int height, int ctr_x, int ctr_y)
@findex KernelRectangle on SomLayerSpec
This makes a rectangular kernel with the given dimensions and center.
@item KernelFromNetView(NetView* view)
@findex KernelFromNetView on SomLayerSpec
This makes a kernel based on the currently selected units in the NetView
(@pxref{net-view}).  Select the center of the kernel first, followed by
the other elements.  Then call this function.
@item StepKernelActs(float val)
@findex StepKernelActs on SomLayerSpec
This assigns the @code{act_val} values of the existing kernel elements
to be all the same value, @code{val}.
@item LinearKernelActs(float scale)
@findex LinearKernelActs on SomLayerSpec
This assigns the @code{act_val} values of the existing kernel elements
as a linear function of their distance from the center, scaled by the
given scale parameter.
@item GaussianKernelActs(float scale, float sigma)
@findex GaussianKernelActs on SomLayerSpec
This assigns the @code{act_val} values of the existing kernel elements
as a Gaussian function of their distance from the center, scaled by the
given scale parameter, where the Gaussian has a variance of @code{sigma}.
@end table

One can see the resulting kernel function by testing the network and
viewing activations.  Also, there is a demo project that illustrates how
to set up a SOM network in @file{demo/so/som.proj.gz}.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  so-proc, so-impl, so-unit, so
@section The So Trial Process

The only process defined for the self-organizing algorithms is a trial
process which simply performs a feed-forward update of the unit's net
input and activations.  Thus, the layers in the network must be ordered
in the order of their dependencies, with later layers depending on
input from earlier ones.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  so-impl,  , so-proc, so
@section So Implementational Details

The following is a chart describing the flow of processing in the So
algorithm, starting with the epoch process, since higher levels do not
interact with the details of particular algorithms:

@example
@group
EpochProcess: @{
  Init: @{
    environment->InitEvents();          // init events (if dynamic)
    event_list.Add() 0 to environment->EventCount(); // get list of events
    if(order == PERMUTE) event_list.Permute();       // permute if necessary
    GetCurEvent();                      // get pointer to current event
  @}
  Loop (trial): @{                      // loop over trials
    SoTrial: @{                         // trial process (one event)
      Init: @{                          // at start of trial
        cur_event = epoch_proc->cur_event; // get cur event from epoch
      @}
      Loop (once): @{                   // only process this once per trial
        network->InitExterns();         // init external inputs to units
        cur_event->ApplyPatterns(network); // apply patterns to network
        Compute_Act(): @{               // compute the activations
          network->layers: @{           // loop over layers
            layer->Compute_Net();       // compute net inputs
            layer->Compute_Act();       // compute activations from net in
          @}
        @}
        network->Compute_dWt();         // compute weight changes from acts
      @}
    @}
    if(wt_update == ON_LINE or wt_update == SMALL_BATCH and trial.val % batch_n)
      network->UpdateWeights(); // after trial, update weights if necc
    GetCurEvent();              // get next event
  @}
  Final:
    if(wt_update == BATCH)  network->UpdateWeights(); // batch weight updates
@}
@end group
@end example

The @code{layer->Compute_Act()} function has several sub-stages for
different versions of algorithms, as detailed below:

For non-input layer hard competitive learning units:
@example
@group
ClLayerSpec::Compute_Act() @{
  SoUnit* win_u = FindWinner(lay);
  float lvcnt = (float)lay->units.leaves;
  lay->avg_act =  // compute avg act assuming one winner and rest losers..
        (act_range.max / lvcnt) + (((lvcnt - 1) * act_range.min) / lvcnt);
  win_u->act = act_range.max; // winning unit gets max value
  win_u->act_i = lay->avg_act;	// and average value goes in _i 
@}
@end group
@end example

For non-input layer soft competitive learning units:
@example
@group
SoftClLayerSpec::Compute_Act() @{
  float sum = 0;
  for(units) @{                 // iterate over the units 
    unit->Compute_Act();                        // compute based on netin
    unit->act = exp(softmax_gain * unit->act);  // then exponential
    sum += unit->act;                           // collect sum
  @}
  for(units) @{                 // then make a second pass
    unit->act =                 // act is now normalized by sum
        act_range.min + act_range.range * (unit->act / sum);
  @}
  Compute_AvgAct();             // then compute average act over layer
@}
@end group
@end example

The code for the SOM case is more complicated than the description,
which is just that it finds the winner and pastes the kernel onto the
units surrounding the winner.
