@c uncomment the following two lines for 'update every node' command
@c @node  over
@c @chapter Conceptual Overview

This chapter provides a general introduction to the PDP++ software and
conceptual overview of its major components.  The primary objective here
is to explain the important design decisions that went into the
software, so that you will be able to get the most out of it.  This is
oriented towards those who have some familiarity with neural networks,
and who want to find out how things are done in this software.  However,
it is also useful for all users to be aware of the rationale for the way
things are set up, so that it becomes clearer where to look for ways to
solve problems that might arise, or accomplish various simulation
objectives. 

@menu
* over-objs::                   Overview of Object-Oriented Software Design
* over-stru::                   Overview of the Main Object Types
* over-spec::                   Separation of State From Specification Variables
* over-scripts::                Scripts: General Extensibility
* over-group::                  Groups: A General Mechanism for Defining Substructure
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-objs, over-stru, over, over
@section Overview of Object-Oriented Software Design
@cindex Object Oriented Software
@cindex Objects, Overview

The PDP++ software is based on the principles of object-oriented
software design.  The effects of this are pervasive, and an
understanding of these basic principles can help the user to understand
the software better.

First of all, everything in the software is an object: units,
connections, layers, networks, etc.  While this sounds intuitive for
these kinds of objects, other kinds of objects, like the object that
controls the way a network is viewed, or the ones that control the way
the network is trained, are somewhat less intuitive.  However, if you
just think of an object as representing some discrete piece or aspect of
the software, it makes more sense.  Essentially the PDP++ software is
just a big collection of objects that work together to let you get
things done.

The primary advantage of an object-oriented design is its
@emph{flexibility}.  Because the functionality is broken up into
object-sized pieces, these pieces can be combined in many different ways
to accomplish many different tasks.  However, this power comes at the
price of @emph{simplicity}---since everything is accomplished by the
interactions between multiple objects, setting things up to perform any
given task requires knowing which objects to use and how to configure
them.  The very distributed nature of the processing makes it less easy
to identify one single thing that needs to be operated on in order to do
something.  Its like the difference between a neural network and a
conventional computer program, in some sense.  In a computer program,
things proceed in a relatively linear, straightforward fashion from one
step to the next, whereas in a neural network, processing is distributed
among many different units that all work together to get something done.

While the tradeoff between flexibility and simplicity is inevitable to a
certain extent, we have endeavored to make the commonly-used tasks
simpler by including menus or buttons that do all of the right things in
one simple step.  However, as you become a more advanced user and want
to perform tasks that have not been "pre-compiled" in this way, expect
to encounter some of the difficulties associated with object-oriented
software.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-stru, over-spec, over-objs, over
@section Overview of the Main Object Types
@cindex Objects, Main Types
@cindex Project
@cindex Network
@cindex Environment
@cindex Logging

This section describes the main ways in which the many tasks involved in
neural network simulation have been divided up and assigned to different
types of objects.  It is essential to understand this division of labor
in order to use the software effectively.

There are four central components to a neural network simulation as we
see it:
@itemize @bullet
@item The @b{Network}, including layers, units, connections, etc.
(@pxref{net}).
@item The @b{Environment} on which the network is to be trained and
tested (@pxref{env}).
@item The @b{Processing} or @b{Scheduling} of training and testing a
network, which determines how long to train the network, with what
environment, etc. (@pxref{proc}).
@item The @b{Logging} of the results of training and testing.  One
can view these results as a graph, a table of numbers, or a grid of
color chips that represent values in a graphical similar to that used in
the viewing of networks (@pxref{log}).
@end itemize

Since all of these components work together in a given simulation, we
have grouped them all under a @b{Project} object (@pxref{proj}), which
also has some additional objects (defaults and scripts) that make life
easier.  Thus, when you start a simulation, the first thing you do is
create a new project, and then start creating networks, environments,
etc. within this project.

@b{New for 2.0:} There is now a graphical interface for the
project-level organization of objects, which can really help make
complex projects more managable, especially for dealing with the
processing aspects of things.  Just enlarge your project window on
existing projects to see the new view.

@b{New for 3.0:} The complexity of dealing with distributed objects is
partially simplified through the introduction of a @b{Wizard} object
that automates the contstruction of typically-used network simulation
objects.  See @ref{how-wizard} for details.  Also new are principal
components analysis (PCA) and multidimensional scaling (MDS)
techniques for analyzing network representations
(@pxref{env-analyze}), and much imporoved graph log features for
displaying overlapping traces of data, including a spike raster plot.
In addition, distributed memory parallel processing support, both for
connection-level and event-level parallelism, was added.

@cindex Processing/Scheduling (Training, Testing, etc)
@cindex Scheduling/Processing (Training, Testing, etc)
@cindex SchedProcess

Of the different simulation components, Processing (aka Scheduling) is
probably the least intuitive.  Indeed, we might have decided instead
that networks knew how to train themselves, or that environments would
know how to train networks.  The main reasons we chose to make
Processing a separate major category of function in the software are: 1)
The ways in which one trains and tests a network are common to different
types of networks and learning algorithms. 2) Many different kinds of
training and testing processing can be performed on the same network and
environment.  3) Processing depends in part on the nature of the network
and the nature of the environment, so that putting this task in either
of these separately would lead to strange dependencies of networks on
environments or vice versa.

Thus, one can think of the processing function as being similar to that
of a movie director, who coordinates the actors and the camera
operators, set designers, etc. to produce a finished product.
Similarly, the processing coordinates the network with the environment
and other elements to direct the overall process of training and
testing.  For more information on how processing is implemented in the
software see @ref{proc-sched}.

@c --------------------------
@menu
* over-stru-net::               The Objects in Networks
* over-stru-env::               The Objects in Environments
* over-stru-proc::              The Objects Involved in Scheduling
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-stru-net, over-stru-env, over-stru, over-stru
@subsection The Objects in Networks
@cindex Projection
@cindex Network (Objects)

For the most part, the objects that make up a network are fairly
intuitive, consisting of Layers (@pxref{net-layer}), Units
(@pxref{net-unit}), and Connections (@pxref{net-con}).  However,
there is an additional type of object which describes the pattern of
connectivity between all of the units in one layer to those in another
layer, which is called a Projection (@pxref{net-prjn}).  The
Projection is very useful because it allows one to specify connectivity
at a broad level, instead of individually connecting each unit with
other units.  It also seems to be a conceptually real entity---one often
thinks about the connectivity in terms like, "the hidden layer is fully
connected to the input layer."

The use of Projection objects is illustrative of a general principle
regarding network structure, which is that the larger-scale objects
(Layers and Projections) contain @emph{parameters} for how to construct
the smaller-scale objects (Units and Connections).  This allows one to
create Layers and Projections, fill in some parameters regarding the
type of connectivity and number and type of units, and execute
general-purpose @code{Build} and @code{Connect} (@pxref{net-net})
functions which translate the parameters into an actual network with
interconnected units.

While it is possible to manually build a network unit-by-unit and
connection-by-connection, it is usually much easier to use the
parameters-and-build/connect method.  However, some people find the
distinction between specifying and actually building difficult to work
with, in part because only after you execute @code{Build} and
@code{Connect} does your actual network reflect the parameters you have
entered.  We have tried to lessen these difficulties by providing
graphical representations of the parameters, so that you can see what
you are specifying at the time you do it, not only after you have built
the network.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-stru-env, over-stru-proc, over-stru-net, over-stru
@subsection The Objects in Environments
@cindex Environment
@cindex Events
@cindex Patterns

We decided to call the training and testing data that is presented to a
network an @code{Environment} (@pxref{env}) to capture the idea that a
network is like an organism that exists and interacts within a
particular environment.  Thus, the environment object describes a little
world for the network to roam around in.  It does not, however, specify
the path that the network takes in its travels (i.e., the order in which
the events are presented to the network)---this is determined by the
scheduling objects, which can be thought of as "tour guides" in this
context.  To switch metaphors, an environment should be thought of as a
library, which contains lots of data, but does not tell you which books
to check out when.

The @code{Environment} is composed of @code{Event}s (@pxref{env-event}),
which represent a collection of stimuli or @code{Pattern}s that go
together.  Thus, in the context of a typical backpropagation network, an
@code{Event} consists of an input pattern and a target pattern for a
single training example.  @code{Pattern}s contain an array of values
which get mapped onto the units in a particular layer of the network.

In order to represent @emph{sequences} of events that should be
presented sequentially in time, events can be grouped together
(@pxref{env-seq}, @pxref{over-group}).  Note, however, that the
scheduling objects need to be made aware of the fact that the events in
a group represent a sequence of events in order for these to actually be
presented to the network in the right order (@pxref{proc-special-seq}).

Environments can also be defined which dynamically create events
on-line, possibly in response to outputs from the network.  This must be
done under the supervision of an appropriate Schedule process, which can
coordinate network outputs with the parameters that determine how the
environment creates events.

@b{New for 2.0:} The Environment interface has been completely
rewritten, and now enables interactive configuration of event
structures, just like in the Network interface.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-stru-proc,  , over-stru-env, over-stru
@subsection The Objects Involved in Processing/Scheduling
@cindex Scheduling
@cindex Schedule Processes
@cindex Statistics
@cindex Process Hierarchy

In order to make the scheduling aspect of the software reasonably
flexible, it has itself been decomposed into a set of different object
types.  Each of these object types plays a specific role within the
overall task of scheduling the training and testing of a network.

The backbone of the scheduling function is made up of a structure
hierarchy of objects of the type @code{SchedProcess}
(@pxref{proc-sched}), which reflects the hierarchy of the different time
grains of processing in a neural network:

@example
@group
Training (loop over epochs)
    Epoch (loop over trials)
        Trial (present a single pattern)
        .
        .
    .
    .
@end group
@end example

Thus, there is one object for every grain of processing, and each
process essentially just loops over the next-finer grain of processing
in the hierarchy.  Because each grain of processing is controlled by a
separate process, it is possible to alter both the parameters and type
of processing that takes place at each grain @emph{independently}.  This
is why this design results in great flexibility.

Since this time grain structure forms the backbone of scheduling, all
other aspects of scheduling, and anything that depends on time grain
information in any way, are oriented around it.  For example, setting
the training criteria becomes embedded in the Training level of
processing.  The updating of displays is associated with a particular
time grain of processing (e.g., telling the system that you want to see
the activations updated in your view of a network after every trial
means associating the view with the Trial grain of processing).
Similarly, the generation of information to be logged, and the update of
the display of this information, is tied to a particular time grain.

The other principal type of object used for scheduling is the
@emph{statistic} (of type @code{Stat}, @pxref{proc-stat}), which is
responsible for computing (or simply recording) data of any kind that is
either relevant for controlling scheduling (e.g., stopping criteria), or
is to be displayed in a log at a particular time grain.  Thus,
statistics are the source of any data that is to be logged, and provide
a general way of specifying when to stop processing based on the values
of computed variables (i.e., sum-squared error).  Again, because these
statistic objects can be mixed and matched with all sorts of different
schedule processes, this creates a high level of flexibility.

Because they are intimately tied to the time grain of processing, and
can directly affect its course, statistics actually "live" on schedule
processes.  Thus, whenever a statistic is created, it is created in a
particular process that governs a given time grain of processing.
Furthermore, statistics can create copies of themselves that reflect
their @emph{aggregation} over higher levels of processing.  For example,
the sum-squared error statistic (@pxref{proc-stats-se}) is created at
the Trial level of processing, where it can actually compute the
difference between the output and target for a given pattern right after
it has been presented.  However, in order for these individual
pattern-level sse values to be accumulated or aggregated over time,
there is another sse statistic at the Epoch level of processing whose
only job is to add up the Trial level sse values.


@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-spec, over-scripts, over-stru, over
@section Separation of State from Specification Variables
@cindex Specifications
@cindex Spec Objects
@cindex State vs. Specification

Another major distinction that is made in the software reflects the
separation of @emph{state} variables (e.g., the current activation of a
unit) from @emph{specification} variables (e.g., the activation function
to be used in computing activation states, and its associated
parameters).  This distinction is made because it is often desirable to
have many objects use the same specifications, while each retain a
distinct set of state variables.  For example, all of the units in one
layer might use a particular learning rate, while units in another layer
have a different one.  By separating these specifications from the state
variables, it is easy to accomplish this.

Thus, associated with many different kinds of objects is a corresponding
@code{Spec} object (e.g., a @code{UnitSpec} for @code{Unit} objects,
etc).  While this separation does sometimes make it a little more
difficult to find the parameters which are controlling a given object's
behavior, it makes the software much more flexible.  Furthermore, we
have put most of the specification objects (specs) in one place, so it
should soon become second nature to look in this place to set or change
parameters.  For more information about specs, see @ref{obj-spec}.

Specs are also used to control the layout of events and patterns in the
environment (throught the use of @code{EventSpec} and @code{PatternSpec}
objects).  Thus, one first configures these specs, and then creates
events according to them.  A new interface makes configuring these specs
much easier than before.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-scripts, over-group, over-spec, over
@section Scripts: General Extensibility
@cindex Scripts
@cindex Script-based Objects
@cindex Recording Scripts
@cindex Scripts, Recording

It is impossible to anticipate all of the things that a user might want
to do in running and controlling a simulation.  For this reason, we have
built a powerful script language into the PDP++ software.  This script
language, called CSS for C-Super-Script or C^C, is essentially a C/C++
interpreter, and thus does not require the user to learn any new syntax
apart from that which would be required to program additions to the
software itself.  This shared syntax means that code developed initially
in the script language can be compiled into the software later for
faster performance, without requiring significant modifications.

CSS scripts are used in several places throughout the software.  They
have been added to some of the basic objects to enable their function to
be defined and extended at run time.  Examples of these include a
@b{ScriptEnv}, which allows an @b{Environment} to be defined dynamically
and have events that are generated on the fly, potentially in response
to the output of the network itself (@pxref{env-other}).  The
@b{ScriptPrjnSpec} allows arbitrary patterns of connectivity to be
defined (@pxref{net-prjn-misc}).  The @b{ScriptStat} allows arbitrary
statistics to be computed and recorded in the data logs during training
and testing (@pxref{proc-stats-misc}).

In addition to these targeted uses, scripts can be used to perform
miscellaneous tasks or repetitive functions at any level of the
software.  It is possible to attach a set of scripts to a given process,
run them automatically when the process starts up, and even record
actions performed in the user interface as script code that can be
replayed later, edited for other uses, etc.  For more information on
these objects, see @ref{proj-scripts}.

Finally, the script language provides a means for interacting with a
simulation when the graphical interface cannot be used, such as over
dial-up lines when working from home, etc.  Anything that can be done in
the GUI can be done in the script language, although it is not quite as
easy.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  over-group,  , over-scripts, over
@section Groups: A General Mechanism for Defining Substructure
@cindex Groups
@cindex Substructure

Another way in which the PDP++ software has built-in flexibility is in
the ability to create substructure at any level.  By substructure, we
mean the grouping of objects together in ways that reflect additional
structure not captured in the basic structural distinctions made in the
software (e.g., as described in the previous sections).  For example, it
is possible to group events together to reflect sequential information.
Also, one can imagine that certain layers should be grouped together to
reflect the fact that they all perform a similar function, and should be
treated as a group. Similarly, units within a layer can be divided into
subgroups that might reflect different parameter settings, etc.

The basic operations of the software are written so as to be insensitive
to this additional substructure (i.e., they can "flatten out" the
groups), which allows the substructure to be used without requiring
special-case code to handle it.

Substructure is defined by creating subgroups of objects.  Thus,
everywhere the user has the opportunity to create an object of a given
type, they also have the opportunity to create a subgroup of such
objects.

For more information, see @ref{obj-group}.
