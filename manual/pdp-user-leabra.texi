@c uncomment the following two lines for 'update every node' command
@c @node  leabra
@c @chapter Leabra

@cindex Leabra

Leabra stands for ``Local, Error-driven and Associative, Biologically
Realistic Algorithm'', and it implements a balance between Hebbian and
error-driven learning on top of a biologically-based point-neuron
activation function with inhibitory competition dynamics (either via
inhibitory interneurons or a fast k-Winners-Take-All approximation
thereof).  Extensive documentation is available from the book:
``Computational Explorations in Cognitive Neuroscience: Understanding
the Mind by Simulating the Brain'', O'Reilly and Munakata, 2000,
Cambridge, MA: MIT Press.  For more information, see the website:
@code{http://psych.colorado.edu/~oreilly/comp_ex_cog_neuro.html}.

Hebbian learning is performed using conditional principal
components analysis (CPCA) algorithm with correction factor for
sparse expected activity levels.

Error driven learning is performed using GeneRec, which is a
generalization of the Recirculation algorithm, and approximates
Almeida-Pineda recurrent backprop.  The symmetric, midpoint version
of GeneRec is used, which is equivalent to the contrastive Hebbian
learning algorithm (CHL).  See @cite{O'Reilly (1996; Neural Computation)}
for more details.

The activation function is a point-neuron approximation with both
discrete spiking and continuous rate-code output.

Layer or unit-group level inhibition can be computed directly using
a k-winners-take-all (KWTA) function, producing sparse distributed
representations, or via inihibitory interneurons.

The net input is computed as an average, not a sum, over
connections, based on normalized, sigmoidaly transformed weight
values, which are subject to scaling on a connection-group level to
alter relative contributions.  Automatic scaling is performed to
compensate for differences in expected activity level in the
different projections.

Weights are subject to a contrast enhancement function, which
compensates for the soft (exponential) weight bounding that keeps
weights within the normalized 0-1 range.  Contrast enhancement is
important for enhancing the selectivity of self-organizing learning,
and generally results in faster learning with better overall results.
Learning operates on the underlying internal linear weight value,
which is computed from the nonlinear (sigmoidal) weight value prior to
making weight changes, and is then converted back.  The linear weight
is always stored as a negative value, so that shared weights or
multiple weight updates do not try to linearize the already-linear
value.  The learning rules have been updated to assume that wt is
negative (and linear).

There are various extensions to the algorithm that implement things
like reinforcement learning (temporal differences), simple recurrent
network (SRN) context layers, and combinations thereof (including an
experimental versions of a complex temporal learning mechanism based on
the prefrontal cortex and basal ganglia).  Other extensions include a
variety of options for the activation and inhibition functions, self
regulation (accommodation and hysteresis, and activity regulation for
preventing overactive and underactive units), synaptic depression, and
various optional learning mechanisms and means of adapting
parameters.  These features are off by default but do appear in some
of the edit dialogs --- any change from default parameters should be
evident in the edit dialogs.

@menu
* leabra-over::                     Overview of the Leabra Algorithm
* leabra-con::                      Leabra Connection Specifications
* leabra-unit::                     Leabra Unit Specifications
* leabra-layer::                    Leabra Layer Specifications
* leabra-proc::                     Leabra Processes
* leabra-stats::                    Leabra Statistics
* leabra-defs::                     Leabra Defaults
* leabra-misc::                     Leabra Misc Special Classes
* leabra-impl::                     Leabra Implementation Details
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-over, leabra-con, leabra, leabra
@section Overview of the Leabra Algorithm

The pseudocode for Leabra is given here, showing exactly how the
pieces of the algorithm described in more detail in the subsequent
sections fit together.

@example
@group
Iterate over minus and plus phases of settling for each event.
  o At start of settling, for all units:
    - Initialize all state variables (activation, v_m, etc).
    - Apply external patterns (clamp input in minus, input & output in
      plus).
    - Compute net input scaling terms (constants, computed
      here so network can be dynamically altered).
    - Optimization: compute net input once from all static activations
      (e.g., hard-clamped external inputs).
  o During each cycle of settling, for all non-clamped units:
    - Compute excitatory netinput (g_e(t), aka eta_j or net)
       -- sender-based optimization by ignoring inactives.
    - Compute kWTA inhibition for each layer, based on g_i^Q:
      * Sort units into two groups based on g_i^Q: top k and
        remaining k+1 -> n.
      * If basic, find k and k+1th highest
        If avg-based, compute avg of 1 -> k & k+1 -> n.
      * Set inhibitory conductance g_i from g^Q_k and g^Q_k+1
    - Compute point-neuron activation combining excitatory input and
      inhibition
  o After settling, for all units, record final settling activations
    as either minus or plus phase (y^-_j or y^+_j).
After both phases update the weights (based on linear current
    weight values), for all connections:
  o Compute error-driven weight changes with soft weight bounding
  o Compute Hebbian weight changes from plus-phase activations
  o Compute net weight change as weighted sum of error-driven and Hebbian
  o Increment the weights according to net weight change.
@end group
@end example

@subsection Point Neuron Activation Function 

@example
@group
Default parameter values:

Parameter | Value | Parameter | Value
--------------------------------------------
E_l       | 0.15  | gbar_l     | 0.10
E_i       | 0.15  | gbar_i     | 1.0
E_e       | 1.00  | gbar_e     | 1.0 
V_rest    | 0.15  | Theta      | 0.25
tau       | .02   | gamma      | 600
k_hebb    | .02   | epsilon    | .01
@end group
@end example

Leabra uses a @emph{point neuron} activation function that models the
electrophysiological properties of real neurons, while simplifying
their geometry to a single point.  This function is nearly as simple
computationally as the standard sigmoidal activation function, but the
more biologically-based implementation makes it considerably easier to
model inhibitory competition, as described below.  Further, using this
function enables cognitive models to be more easily related to more
physiologically detailed simulations, thereby facilitating
bridge-building between biology and cognition.

The membrane potential @code{V_m} is updated as a function of ionic
conductances @code{g} with reversal (driving) potentials @code{E} as
follows:

@iftex
@tex
% html Delta V_m(t) = \tau sum_c g_c(t) gbar_c (E_c - V_m(t))
$$   \Delta V_m(t) = \tau \sum_c g_c(t) \overline{g_c} (E_c - V_m(t)) $$	
@end tex
@end iftex
@ifinfo
     Delta V_m(t) = \tau sum_c g_c(t) gbar_c (E_c - V_m(t))
@end ifinfo

with 3 channels (c) corresponding to: e excitatory input; l leak
current; and i inhibitory input.  Following electrophysiological
convention, the overall conductance is decomposed into a time-varying
component g_c(t) computed as a function of the dynamic state of the
network, and a constant gbar_c that controls the relative
influence of the different conductances.  The equilibrium potential
can be written in a simplified form by setting the excitatory driving
potential (E_e) to 1 and the leak and inhibitory driving potentials
(E_l and E_i) of 0:

@iftex
@tex
% html V_m^infty = [g_e gbar_e] / [g_e gbar_e + g_l gbar_l + g_i gbar_i]
$$ V_m^{\infty} = {{g_e \overline{g_e}} \over {g_e
   \overline{g_e} + g_l \overline{g_l} + g_i \overline{g_i}}} $$	
@end tex
@end iftex
@ifinfo
     V_m^infty = [g_e gbar_e] / [g_e gbar_e + g_l gbar_l + g_i gbar_i]
@end ifinfo

which shows that the neuron is computing a balance between excitation
and the opposing forces of leak and inhibition.  This equilibrium form
of the equation can be understood in terms of a Bayesian decision
making framework @cite{(O'Reilly & Munakata, 2000)}.

The excitatory net input/conductance g_e(t) or eta_j is computed
as the proportion of open excitatory channels as a function of sending
activations times the weight values:

@iftex
@tex
% html eta_j = g_e(t) = < x_i w_ij > = 1/n sum_i x_i w_ij
$$ \eta_j = g_e(t) = \langle x_i w_{ij} \rangle = {{1} \over {n}} \sum_i x_i w_{ij} $$	
@end tex
@end iftex
@ifinfo
        eta_j = g_e(t) = < x_i w_ij > = 1/n sum_i x_i w_ij     
@end ifinfo

The inhibitory conductance is computed via the kWTA function described
in the next section, and leak is a constant.

Activation communicated to other cells (y_j) is a thresholded
(Theta) sigmoidal function of the membrane potential with gain
parameter gamma:

@iftex
@tex
% html y_j(t) = 1 / (1 + 1/(\gamma [V_m(t) - Theta]_+))
$$ y_j(t) = {{1} \over {\left(1 + {{1} \over {\gamma [V_m(t) - \Theta]_+}} \right)}} $$	
@end tex
@end iftex
@ifinfo
        y_j(t) = 1 / (1 + 1/(\gamma [V_m(t) - Theta]_+))     
@end ifinfo

where [x]_+ is a threshold function that returns 0 if x<0 and x
if X>0.  Note that if it returns 0, we assume y_j(t) = 0, to avoid
dividing by 0.  As it is, this function has a very sharp threshold,
which interferes with graded learning learning mechanisms (e.g.,
gradient descent).  To produce a less discontinuous deterministic
function with a softer threshold, the function is convolved with a
Gaussian noise kernel (\mu=0, \sigma=.005), which reflects the
intrinsic processing noise of biological neurons:

@iftex
@tex
% html y^*_j(x) = int_-infty^infty 1 / (sqrt(2 pi) sigma) exp(-z^2/(2 sigma^2)) y_j(z-x) dz
$$ y^*_j(x) = \int_{-\infty}^{\infty} {{1} \over {\sqrt{2 \pi} \sigma}}
  e^{-z^2/(2 \sigma^2)} y_j(z-x) dz $$	
@end tex
@end iftex
@ifinfo
        y^*_j(x) = int_-infty^infty 1 / (sqrt(2 pi) sigma) exp(-z^2/(2 sigma^2)) y_j(z-x) dz     
@end ifinfo
  
where x represents the [V_m(t) - \Theta]_+ value, and y^*_j(x)
is the noise-convolved activation for that value.  In the simulation,
this function is implemented using a numerical lookup table.

@subsection k-Winners-Take-All Inhibition

Leabra uses a kWTA (k-Winners-Take-All) function to achieve inhibitory
competition among units within a layer (area).  The kWTA function
computes a uniform level of inhibitory current for all units in the
layer, such that the k+1th most excited unit within a layer is
generally below its firing threshold, while the kth is typically
above threshold.  Activation dynamics similar to those produced by the
kWTA function have been shown to result from simulated inhibitory
interneurons that project both feedforward and feedback inhibition
(OReilly & Munakata, 2000).  Thus, although the kWTA function is
somewhat biologically implausible in its implementation (e.g.,
requiring global information about activation states and using sorting
mechanisms), it provides a computationally effective approximation to
biologically plausible inhibitory dynamics.

kWTA is computed via a uniform level of inhibitory current for all
units in the layer as follows:

@iftex
@tex
% html g_i = g^Theta_k+1 + q (g^Theta_k - g^Theta_k+1)
$$ g_i = g^{\Theta}_{k+1} + q (g^{\Theta}_k - g^{\Theta}_{k+1}) $$	
@end tex
@end iftex
@ifinfo
        g_i = g^Theta_k+1 + q (g^Theta_k - g^Theta_k+1)     
@end ifinfo

where 0<q<1 (.25 default used here) is a parameter for setting the
inhibition between the upper bound of g^Theta_k and the lower
bound of g^Theta_k+1.  These boundary inhibition values are
computed as a function of the level of inhibition necessary to keep a
unit right at threshold:

@iftex
@tex
% html g_i^Theta = (g^*_e gbar_e (E_e - Theta) + g_l gbar_l (E_l - Theta)) / (Theta - E_i)
$$ g_i^{\Theta} = {{g^*_e \bar{g_e} (E_e - \Theta) +
    g_l \bar{g_l} (E_l - \Theta)} \over {\Theta - E_i}} $$	
@end tex
@end iftex
@ifinfo
        g_i^Theta = (g^*_e gbar_e (E_e - Theta) + g_l gbar_l (E_l - Theta)) / (Theta - E_i)     
@end ifinfo

where g^*_e is the excitatory net input without the bias weight
contribution --- this allows the bias weights to override the kWTA
constraint.

In the basic version of the kWTA function, which is relatively rigid
about the kWTA constraint and is therefore used for output layers,
g^Theta_k and g^Theta_k+1 are set to the threshold
inhibition value for the kth and k+1th most excited units,
respectively.  Thus, the inhibition is placed exactly to allow k
units to be above threshold, and the remainder below threshold.  For
this version, the q parameter is almost always .25, allowing the
kth unit to be sufficiently above the inhibitory threshold.

In the @emph{average-based} kWTA version, g^Theta_k is the average
g_i^Theta value for the top k most excited units, and
g^Theta_k+1 is the average of g_i^Theta for the remaining
n-k units.  This version allows for more flexibility in the actual
number of units active depending on the nature of the activation
distribution in the layer and the value of the q parameter (which is
typically .6), and is therefore used for hidden layers.

@subsection Hebbian and Error-Driven Learning

For learning, Leabra uses a combination of error-driven and Hebbian
learning.  The error-driven component is the symmetric midpoint
version of the GeneRec algorithm @cite{(O'Reilly, 1996)}, which is
functionally equivalent to the deterministic Boltzmann machine and
contrastive Hebbian learning (CHL).  The network settles in two
phases, an expectation (minus) phase where the network's actual output
is produced, and an outcome (plus) phase where the target output is
experienced, and then computes a simple difference of a pre and
postsynaptic activation product across these two phases.  For Hebbian
learning, Leabra uses essentially the same learning rule used in
competitive learning or mixtures-of-Gaussians which can be seen as a
variant of the Oja normalization @cite{(Oja, 1982)}.  The error-driven and
Hebbian learning components are combined additively at each connection
to produce a net weight change.

The equation for the Hebbian weight change is:

@iftex
@tex
% html Delta_hebb w_ij = x^+_i y^+_j - y^+_j w_ij = y^+_j (x^+_i - w_ij)
$$ \Delta_{hebb} w_{ij} = x^+_i y^+_j - y^+_j w_{ij} = y^+_j (x^+_i - w_{ij}) $$	
@end tex
@end iftex
@ifinfo
        Delta_hebb w_ij = x^+_i y^+_j - y^+_j w_ij = y^+_j (x^+_i - w_ij)     
@end ifinfo

and for error-driven learning using CHL:

@iftex
@tex
% html Delta_err w_ij = (x^+_i y^+_j) - (x^-_i y^-_j)
$$ \Delta_{err} w_{ij} = (x^+_i y^+_j) - (x^-_i y^-_j) $$	
@end tex
@end iftex
@ifinfo
        Delta_err w_ij = (x^+_i y^+_j) - (x^-_i y^-_j)     
@end ifinfo

which is subject to a soft-weight bounding to keep within the 0-1 range:

@iftex
@tex
% html Delta_sberr w_ij = [Delta_err]_+ (1-w_ij) + [Delta_err]_- w_ij
$$ \Delta_{sberr} w_{ij} = [\Delta_{err}]_+ (1-w_{ij}) + [\Delta_{err}]_- w_{ij} $$	
@end tex
@end iftex
@ifinfo
        Delta_sberr w_ij = [Delta_err]_+ (1-w_ij) + [Delta_err]_- w_ij     
@end ifinfo
  
The two terms are then combined additively with a normalized mixing
constant k_hebb:

@iftex
@tex
% html Delta w_ij = epsilon [k_hebb (Delta_hebb) + (1-k_hebb) (Delta_sberr)]
$$ \Delta w_{ij} = \epsilon[k_{hebb} (\Delta_{hebb}) + (1-k_{hebb}) (\Delta_{sberr})] $$	
@end tex
@end iftex
@ifinfo
        Delta w_ij = epsilon [k_hebb (Delta_hebb) + (1-k_hebb) (Delta_sberr)]     
@end ifinfo

@subsection Implementational Overview

@cindex LayerSpec
The Leabra implementation defines a full range of specialized network
objects, from connections through layers.  Unlike most PDP++
simulations, Leabra makes extensive use of the @b{LayerSpec} type,
specifically the @b{LeabraLayerSpec}, which specifies the
k-winners-take-all inhibition.

The new schedule process objects consist of three essential levels of
processing, starting at the trial level and moving down through settling
to the cycle, which implements one activation update of the network.
Thus, the @b{LeabraTrial} process loops over the plus and minus phases of
settling in the @b{LeabraSettle} process, which in turn iterates over cycles
of the @b{LeabraCycle} process, which updates the activations of the units
in the network. 

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-con, leabra-unit, leabra-over, leabra
@section Leabra Connection Specifications
@cindex Connections, Leabra
@tindex LeabraCon
@tindex LeabraConSpec

The Leabra connection type @b{LeabraCon} contains the following parameters:

@table @code
@item wt
@vindex wt of LeabraCon
The weight value (shows up in NetView as r.wt for receiving, s.wt for sending
@item dwt
@vindex dwt of LeabraCon
Accumulated change in weight value computed for current trial:
this is usually zero by the time NetView is updated
@item pdw
@vindex pdw of LeabraCon
Previous dwt weight change value: this is what is visible in NetView.
@end table

The Leabra connection specification @b{LeabraConSpec} type contains the following parameters:

@tindex LeabraConSpec
@table @code
@item rnd
@vindex rnd of LeabraConSpec
Controls the random initialization of the weights:

@table @code
@item type
@vindex type of LeabraConSpec
Select type of random distribution to use (e.g., UNIFORM (default), NORMAL (Gaussian)).

@item mean
@vindex mean of LeabraConSpec
Mean of the random distribution (mean rnd weight val).
@item var
@vindex var of LeabraConSpec
Variance of the distribution (range for UNIFORM).
@item par
@vindex par of LeabraConSpec
2nd parameter for distributions like BINOMIAL and GAMMA that require it (not typically used).
@end table

@item wt_limits
@vindex wt_limits of LeabraConSpec
Sets limits on the weight values ---Leabra weights are constrained between 0 and 1 and are initialized
    to be symmetric:

@table @code
@item type
@vindex type of LeabraConSpec
Type of constraint (GT_MIN = greater than
    min, LT_MAX = less than max, MIN_MAX (default) within both min
    and max)

@item min
@vindex min of LeabraConSpec
Minimum weight value (if GT_MIN or MIN_MAX).
@item max
@vindex max of LeabraConSpec
Maximum weight value (if LT_MAX or MIN_MAX).
@item sym
@vindex sym of LeabraConSpec
Symmetrizes the weights (only done at initialization).
@end table

@item inhib
@vindex inhib of LeabraConSpec
Makes the connection inhibitory (net input goes to g_i instead of net).

@item wt_scale
@vindex wt_scale of LeabraConSpec
Controls relative and absolute scaling of
    weights from different projections:

@table @code
@item abs
@vindex abs of LeabraConSpec
Absolute scaling (s_k): directly multiplies weight value.
@item rel
@vindex rel of LeabraConSpec
Relative scaling (r_k): effect is normalized by sum of rel values for all incoming projections.
@end table

@item wt_sig
@vindex wt_sig of LeabraConSpec
Parameters for the sigmoidal weight contrast enhancement function:

@table @code
@item gain
@vindex gain of LeabraConSpec
Gain parameter: how sharp is the contrast enhancement. 1=linear function.
@item off
@vindex off of LeabraConSpec
Offset parameter: for values >1, how 
far above .5 is neutral point on contrast enhancement curve
(1=neutral is at .5, values <1 not used, 2 is probably the
maximum usable value).
@end table

@item lrate
@vindex lrate of LeabraConSpec
Learning rate (epsilon).

@item cur_lrate
@vindex cur_lrate of LeabraConSpec
Current learning rate as affected by
@code{lrate_sched}: note that this is only updated when the network is
actually run (and only for ConSpecs that are actually used in
network).
 
@item lrate_sched
@vindex lrate_sched of LeabraConSpec
Schedule of learning rate over training
epochs: to use, create elements in the list, assign start_ctr's
to epoch vals when lrate's (given by start_val's) take effect.
These start_val lrates @emph{multiply} the basic lrate, so use .1
for a cur_lrate of .001 if basic lrate = .01.

@item lmix
@vindex lmix of LeabraConSpec
Sets mixture of Hebbian and err-driven learning:
@table @code

@item hebb
@vindex hebb of LeabraConSpec
Amount of Hebbian learning: unless using
pure Hebb (1), values greater than .05 are usually to big.  For
large networks trained on many patterns, values as low as .00005 are
still useful.
@item err
@vindex err of LeabraConSpec
Amount of error-driven: automatically
set to be 1-hebb, so you can't set this independently.
@end table

@item fix_savg
@vindex fix_savg of LeabraConSpec
Sets fixed sending avg activity
    value for normalizing netin (aka alpha_k): g_e_k = 1 / alpha_k < x_i
    w_ij >_k.  This is useful when expected activity of sending
    region that projection actually receives is different from that of
    sending layer
    as a whole.

@table @code
@item fix
@vindex fix of LeabraConSpec
Toggle for actually fixing the sending avg activation to value set in savg.
@item savg
@vindex savg of LeabraConSpec
The fixed sending average activation value --- should be between 0 and 1.
@item div_gp_n
@vindex div_gp_n of LeabraConSpec
Divide by group n, not layer n,
where group n is the number of actual connections in the
connection group that this unit receives from (corresponds to a
given projection).  Usually, the netinput is averaged by dividing
by layer n, so it is the same even with partial connectivity ---
use this flag to override where projection n is more meaningful.
@end table

@item savg_cor
@vindex savg_cor of LeabraConSpec
Correction for sending average activation
    levels in hebbian learning --- renormalizes weights to use full
    dynamic range even with sparse sending activity levels that would
    otherwise result in generally very small weight values.

@table @code
@item cor
@vindex cor of LeabraConSpec
Amount of correction to apply (0=none, 1=all, .5=half, etc): (aka q_m):
    alpha_m = .5 - q_m (.5 - alpha), where m = .5 / alpha_m),
    and Delta w_ij = epsilon [y_j x_i (m - wij) + y_j (1-x_i)(0 -
    wij)]

@item src
@vindex src of LeabraConSpec
Source of the sending average act for use in
    correction. SLAYER_AVG_ACT (default) = use actual sending layer
    average activation. SLAYER_TRG_PCT = use sending layer target
    activation level.  FIXED_SAVG = use value specified in
    fix_savg.savg.  COMPUTED_SAVG = use actual computed average
    sending activation @emph{for each specific projection} --- this is
    very computationally expensive and almost never used.

@item thresh
@vindex thresh of LeabraConSpec
Threshold of sending average
    activation below which Hebbian learning does not occur --- if the
    sending layer is essentially inactive, it is much faster to simply
    ignore it.  Note that this also has the effect of preserving
    weight values for projections coming from inactive layers, whereas
    they would otherwise uniformly decrease.
@end table
@end table


@tindex LeabraBiasSpec
The @b{LeabraBiasSpec} connection specification is for bias weight (bias
weights do not have the normal weight bounding and @code{wt_limits}
settings, are initialized to zero with zero variance, and do not have
a Hebbian learning component).  The parameters are:

@table @code
@item dwt_thresh
@vindex dwt_thresh of LeabraXXX
Don't change weights if @code{dwt}
  (weight change) is below this value --- this prevents bias weights
  from slowly creeping up or down and growing ad-infinitum even when
  the network is basically performing correctly --- essentially a
  tolerance factor for how accurate the actual activation has to be
  relative to the target.
@end table

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-unit, leabra-layer, leabra-con, leabra
@section Leabra Unit Specifications
@cindex Unit, Leabra
@tindex LeabraUnit
@tindex LeabraUnitSpec

The parameters for the @b{LeabraUnit} unit object are as follows:

@table @code
@item spec
@vindex spec of LeabraUnit
Determines the spec that controls this unit (not in NetView).

@item pos
@vindex pos of LeabraUnit
Determines location of unit within layer (not in NetView).

@item ext_flag
@vindex ext_flag of LeabraUnit
Reflects type of external input to unit (not in NetView)

@item targ
@vindex targ of LeabraUnit
Target activity value (provided by external input from event).

@item ext
@vindex ext of LeabraUnit
External activation value when clamped (provided by external
  input from event).

@item act
@vindex act of LeabraUnit
Activation value (what is sent to other units, y_j).

@item net
@vindex net of LeabraUnit
Net input value (eta_j) computed as normalized weights
  times activations --- excitation only, inhibition is computed
  separately as g_i @code{gc.i} either by kWTA or directly by unit
  inhib.

@item bias
@vindex bias of LeabraUnit
The bias weight is a LeabraCon object hanging
  off of the unit --- it is managed by its own LeabraBiasSpec in the
  LeabraUnitSpec.

@item act_eq
@vindex act_eq of LeabraUnit
Rate-code equivalent activity value
  (time-averaged spikes when using discrete spiking activation, or
  just a copy of @code{act} when already using rate code activation).

@item act_avg
@vindex act_avg of LeabraUnit
Average activation over long time intervals,
  as integrated by time constant in @code{adapt_thr} (see LeabraUnitSpec).
  Useful to see which units are dominating, and to adapt their
  thresholds if that is enabled.

@item act_m
@vindex act_m of LeabraUnit
Minus phase activation value, set after settling in
  minus phase and used for learning.
@item act_p
@vindex act_p of LeabraUnit
Plus phase activation value, set after settling in
  plus phase and used for learning.

@item act_dif
@vindex act_dif of LeabraUnit
Difference between plus and minus phase activations
  --- equivalent to the error contribution for unit (delta_j).

@item da
@vindex da of LeabraUnit
Delta activation: change in activation from one
    cycle to the next, used for determining when to stop settling.

@item vcb
@vindex vcb of LeabraUnit
Voltage-gated channel basis variables that
    integrate activation over time to determine if channels should be
    open or closed (channels are not active by default) -- there are
    two types: hyst and acc, described next, followed by the
    parameters common to both.

@item hyst
@vindex hyst of LeabraUnit
Hysteresis channel (excitatory) basis
  variable --- typically hysteresis is triggered after unit achieves
  brief sustained level of excitation as reflected in this basis
  variable.

@item acc
@vindex acc of LeabraUnit
Accommodation channel (inhibitory) basis
  variable --- typically accommodation (fatigue) is triggered after
  unit is active for a relatively long time period as reflected in
  this more slowly-integrating basis variable.

@table @code
@item gc
@vindex gc of LeabraUnit
Channel conductances for the different input channel
  types except excitatory input (which is in @code{net}).
@item l
@vindex l of LeabraUnit
Leak channel conductance (a constant, not visible in NetView).
@item i
@vindex i of LeabraUnit
Inhibition channel conductance, computed by kWTA or direct unit inhibition.
@item h
@vindex h of LeabraUnit
Hysteresis (voltage-gated excitation) channel conductance.
@item a
@vindex a of LeabraUnit
Accommodation (voltage-gated inhibition) channel conductance.
@end table

@item I_net
@vindex I_net of LeabraUnit
Net current produced by all channels: what drives the changes in membrane potential.
@item v_m
@vindex v_m of LeabraUnit
The membrane potential, integrates over time
  weighted-average inputs across different channels, provides basis
  for activation output via thresholded, saturating nonlinear
  function.

@item i_thr
@vindex i_thr of LeabraUnit
Inhibitory threshold value used in computing kWTA (g_i_Theta).
@item spk_amp
@vindex spk_amp of LeabraUnit
Amplitude of spiking output (for depressing synapse activation function)
@end table

The @b{LeabraUnitSpec} unit-level specifications:

@table @code

@item act_range
@vindex act_range of LeabraUnitSpec
Range of activation for units: Leabra units
  are bounded between 0 (@code{min}) and 1 (@code{max}).

@item bias_con_type
@vindex bias_con_type of LeabraUnitSpec
Type of bias connection to make: almost always LeabraCon.

@item bias_spec
@vindex bias_spec of LeabraUnitSpec
The LeabraBiasSpec that controls the bias connection on the unit.

@item act_fun
@vindex act_fun of LeabraUnitSpec
The activation function to use:
    NOISY_XX1 (default), XX1 (not convolved with noise), LINEAR (@code{act}
    is linear function of @code{v_m} above threshold (0 below threshold)),
  SPIKE (discrete spiking).

@item act
@vindex act of LeabraUnitSpec
Specifications for the activation function:

@table @code
@item thr
@vindex thr of LeabraUnitSpec
The threshold value Theta in: y_j = (gamma [V_m - Theta]_+) / (gamma
  [V_m - Theta]_+ + 1).
@item gain
@vindex gain of LeabraUnitSpec
Gain of the activation function (gamma in: y_j = (gamma [V_m - Theta]_+)(gamma
  [V_m - Theta]_+ + 1). 
@item nvar
@vindex nvar of LeabraUnitSpec
Variance of the Gaussian noise kernel for convolving with XX1 function in NOISY_XX1.
@end table

@item spike
@vindex spike of LeabraUnitSpec
Specifications for the discrete spiking activation function (SPIKE):

@table @code
@item dur
@vindex dur of LeabraUnitSpec
Spike duration in cycles --- models
  extended duration of effect on postsynaptic neuron via opened
  channels, etc.
@item v_m_r
@vindex v_m_r of LeabraUnitSpec
Post-spiking membrane potential to
  reset to, produces a refractory effect and controls overall rate of
  firing (0 std).
@item eq_gain
@vindex eq_gain of LeabraUnitSpec
Gain for computing @code{act_eq}
  relative to actual time-average spiking rate (gamma_eq in: y_j^eq = gamma_eq
  (N_spikes)(N_cycles)).
@item ext_gain
@vindex ext_gain of LeabraUnitSpec
Gain for clamped external inputs,
  multiplies the @code{ext} value before clamping, needed because
  constant external inputs otherwise have too much influence compared to
  spiking ones. 
@end table 

@item act_reg
@vindex act_reg of LeabraUnitSpec
Activity regulation via global weight scaling specifications (not used by default):
@table @code
@item on
@vindex on of LeabraUnitSpec
whether to activity regulation is on (active) or not
@item min
@vindex min of LeabraUnitSpec
Increase weights for units below this level of average activation
@item max
@vindex max of LeabraUnitSpec
Decrease weights for units above this level of average activation 
@item wt_dt
@vindex wt_dt of LeabraUnitSpec
rate constant for making weight changes to rectify over-activation (dwt ~= wt_dt * wt)
@end table

@item opt_thresh
@vindex opt_thresh of LeabraUnitSpec
Optimization thresholds for speeding up computation: 

@table @code
@item send
@vindex send of LeabraUnitSpec
Don't send activation when act <= send.
@item learn
@vindex learn of LeabraUnitSpec
Don't learn on recv unit weights when both phase acts <= learn.
@item updt_wts
@vindex updt_wts of LeabraUnitSpec
Whether to apply @code{learn} threshold to updating weights (otherwise always update).
@item phase_dif
@vindex phase_dif of LeabraUnitSpec
Don't learn when +/- phase
    difference ratio (- / +) < phase_dif.  This is off (0) by
    default, but can be useful if network is failing to activate
    output (e.g., in a deep network) on minus phase of some trials ---
    learning in this case is just massive increase in all weights, and
    tends to produce ``hog'' units for all the active units.  To use,
    set to .8 as a good initial value.
@end table

@item clamp_range
@vindex clamp_range of LeabraUnitSpec
Range of clamped (external)
    activation values (@code{min, max}) --- Don't clamp to 1 because
    NOISY_XX1 activations can't reach that value, so use .95 as max.

@item vm_range
@vindex vm_range of LeabraUnitSpec
Membrane potential range (@code{min, max}), 0-1 for normalized, -90-50 for bio-based.

@item v_m_init
@vindex v_m_init of LeabraUnitSpec
Random distribution for initializing the membrane potential (constant by default). 

@item dt
@vindex dt of LeabraUnitSpec
Time constants for integrating values over time:
@item vm
@vindex vm of LeabraUnitSpec
Membrane potential @code{v_m} time constant: dt_vm in: V_m(t+1) = V_m(t) + dt_vm I_net-.
@item net
@vindex net of LeabraUnitSpec
Net input @code{net} time constant: dt_net
  in: g_e(t) = (1 - dt_net) g_e(t-1) + dt_net ( 1/n_p sum_k g_e_k +
  1 / (beta/N)).

@item g_bar
@vindex g_bar of LeabraUnitSpec
Maximal conductances for channels:

@table @code
@item e
@vindex e of LeabraUnitSpec
Excitatory (glutamatergic synaptic sodium (Na) channel).
@item l
@vindex l of LeabraUnitSpec
Constant leak (potassium, K+) channel.
@item i
@vindex i of LeabraUnitSpec
Inhibitory GABA-ergic channel (computed by kWTA or directly).
@item h
@vindex h of LeabraUnitSpec
Hysteresis (excitation) voltage-gated channel (Ca++).
@item a
@vindex a of LeabraUnitSpec
Accommodation (fatigue, inhibition) voltage-gated channel (K+).
@item e_rev
@vindex e_rev of LeabraUnitSpec
Reversal potentials for each channel (see above, defaults: 1, .15, .15, 1, 0).
@end table

@item hyst
@vindex hyst of LeabraUnitSpec
Hysteresis (excitation) voltage-gated channel
  specs, see accommodation (@code{acc}) for details, defaults are:
  false, .05, .8, .7, .1 and true).

@item acc
@vindex acc of LeabraUnitSpec
Accommodation (fatigue, inhibition) voltage-gated channel:
@table @code
@item on
@vindex on of LeabraUnitSpec
 Activate use of channel if true.
@item b_dt
@vindex b_dt of LeabraUnitSpec
Time constant for integrating basis variable,
  dt_b_a in: b_a(t) = b_a(t-1) +  dt_b_a (y_j(t) - b_a(t-1)).
@item a_thr
@vindex a_thr of LeabraUnitSpec
Activation threshold for basis variable, when
  exceeded opens the channel, aka Theta_a.
@item d_thr
@vindex d_thr of LeabraUnitSpec
Deactivation threshold for basis
  variable, when less than closes channel (after having been opened),
  aka Theta_d.
@item g_dt
@vindex g_dt of LeabraUnitSpec
Time constant for changing conductance when
  activating or deactivating, aka dt_g_a
@item init
@vindex init of LeabraUnitSpec
If true, initialize basis variables when state is initialized (else with weights).
@end table

@item noise_type
@vindex noise_type of LeabraUnitSpec
Where to add noise in the processing (if at all): NO_NOISE (default) =
no noise, VM_NOISE = add to @code{v_m} (most commonly used),
NETIN_NOISE = add to @code{net}, ACT_NOISE = add to activation
@code{act}.
 
@item noise
@vindex noise of LeabraUnitSpec
Distribution parameters for random added
  noise, default = GAUSSIAN, mean = 0, var = .001.
@item noise_sched
@vindex noise_sched of LeabraUnitSpec
Schedule of noise variance over
  settling cycles, can be used to make an @emph{annealing} schedule
  (rarely needed), use same logic as @code{lrate_sched} described in
  LeabraConSpec.
@end table

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-layer, leabra-proc, leabra-unit, leabra
@section Leabra Layer Specifications
@cindex Layer, Leabra
@tindex LeabraLayer
@tindex LeabraLayerSpec

The @b{LeabraLayer} layer object has the following variables:

@table @code
@item n_units
@vindex n_units of LeabraLayer
Number of units to create with Build command (0=use geometry). 
@item geom
@vindex geom of LeabraLayer
Geometry (size) of units in layer (or of each subgroup if geom.z > 1).
@item pos
@vindex pos of LeabraLayer
Position of layer within network.
@item gp_geom
@vindex gp_geom of LeabraLayer
Geometry of subgroups (if geom.z > 1).
@item projections
@vindex projections of LeabraLayer
Group of receiving projections for this layer.
@item units
@vindex units of LeabraLayer
Units or groups of units in the layer.
@item unit_spec
@vindex unit_spec of LeabraLayer
Default unit specification for units in this
    layer: only applied during Build or explicit SetUnitSpec command.
@item lesion
@vindex lesion of LeabraLayer
Inactivate this layer from processing (reversible).
@item ext_flag
@vindex ext_flag of LeabraLayer
Indicates which kind of external input layer received.

@item netin
@vindex netin of LeabraLayer
Average and maximum net input (@code{net}) values for
  layer (@code{avg}, @code{max}).  These values kept for information
  purposes only.
@item acts
@vindex acts of LeabraLayer
Avg and max activation values for the layer --- @code{avg} is used for
sending average activation computation in @code{savg_cor} in the
ConSpec.
@item acts_p
@vindex acts_p of LeabraLayer
Plus-phase activation stats for the layer.
@item acts_m
@vindex acts_m of LeabraLayer
Minus-phase activation stats for the layer.
@item acts_dif
@vindex acts_dif of LeabraLayer
Difference between plus and minus phase vals above.
@item phase_dif_ratio
@vindex phase_dif_ratio of LeabraLayer
Phase-difference ratio (@code{acts_p.avg / acts_m.avg}) that can be
used with @code{phase_dif} in UnitSpec to prevent learning when
network is inactive in minus phase but active in plus phase.
@item kwta
@vindex kwta of LeabraLayer
values for kwta -- activity levels, etc:

@table @code
@item k
@vindex k of LeabraLayer
Actual target number of active units for layer.
@item pct
@vindex pct of LeabraLayer
Actual target percent activity in layer.
@item k_ithr
@vindex k_ithr of LeabraLayer
Inhib threshold for kth most active unit (top k for avg-based). 
@item k1_ithr
@vindex k1_ithr of LeabraLayer
Inhib threshold for k+1th unit (other units for avg-based). 
@item ithr_r
@vindex ithr_r of LeabraLayer
Log of ratio of ithr values, indicates
sharpness of differentiation between active and inactive units.
@end table

@item i_val
@vindex i_val of LeabraLayer
Computed inhibition values: @code{kwta} = kWTA
  inhibition, @code{g_i} = overall inhibition (usually same as kwta,
  but not for UNIT_INHIB). 
@item un_g_i
@vindex un_g_i of LeabraLayer
Average and stdev (not max) values for unit inhib.
@item adapt_pt
@vindex adapt_pt of LeabraLayer
Adapting kwta point values (if adapting, not by default).
  
@item spec
@vindex spec of LeabraLayer
Determines the spec that controls this layer.
@item layer_links
@vindex layer_links of LeabraLayer
List of layers to link inhibition with (not commonly used).
@item stm_gain
@vindex stm_gain of LeabraLayer
Actual stim gain for soft clamping, can be incremented to ensure clamped units active.
@item hard_clamped
@vindex hard_clamped of LeabraLayer
If true, this layer is actually hard clamped.
@end table

The @b{LeabraLayerSpec} layer-level specifications consist of:

@table @code
@item kwta
@vindex kwta of LeabraLayerSpec
How to calculate desired activity level:

@table @code
@item k_from
@vindex k_from of LeabraLayerSpec
How is the actual k determined: USE_K =
  directly by given k, USE_PCT = by pct times number of units in
  layer (default), USE_PAT_K = by number of units where external
  input @code{ext} > @code{.5 (pat_q)}.
@item k
@vindex k of LeabraLayerSpec
Desired number of active units in the layer
  (default is meaningless --- change as appropriate).
@item pct
@vindex pct of LeabraLayerSpec
Desired proportion of activity (used to compute a k value based on layer size).
@end table

@item gp_kwta
@vindex gp_kwta of LeabraLayerSpec
Desired activity level for the unit groups (not
    applicable if no unit subgroups in layer, or if not in inhib_group).
    See @code{kwta} for values. 
@item inhib_group
@vindex inhib_group of LeabraLayerSpec
What to consider the inhibitory group.
    ENTIRE_LAYER = layer (default), UNIT_GROUPS = unit subgroups
    within layer each compute kwta separately, LAY_AND_GPS = do both
    layer and subgroup, inhib is max of each value.

@item compute_i
@vindex compute_i of LeabraLayerSpec
How to compute inhibition (g_i):
  KWTA_INHIB = basic kWTA between k and k+1 (default),
  KWTA_AVG_INHIB = average based, between avg of k and avg of k+1-n,
  UNIT_INHIB = units with @code{inhib} flag send g_i directly.
@item i_kwta_pt
@vindex i_kwta_pt of LeabraLayerSpec
Point to place inhibition between k and
  k+1 for kwta (.25 std), between avg of k and avg of k+1-n for
  avg-based (.6 std).

@item adapt_i
@vindex adapt_i of LeabraLayerSpec
Adapt either the @code{i_kwta_pt} point based on
  difference between actual and target pct activity level (for
  avg-based only, and rarely used), or or g_bar.i for unit-inhib.

@table @code
@item type
@vindex type of LeabraLayerSpec
What type of adaptation: NONE = nothing, KWTA_PT = adapt kwta point
(i_kwta_pt) based on running-average layer activation as compared to
target value, G_BAR_I =  adapt g_bar.i for unit inhibition values
based on layer activation at any point in time, G_BAR_IL adapt g_bar.i
and g_bar.l for unit inhibition & leak values based on layer
activation at any point in time 
@item tol
@vindex tol of LeabraLayerSpec
Tolerance around target before changing value.
@item p_dt
@vindex p_dt of LeabraLayerSpec
Time constant for changing parameter.
@item mx_d
@vindex mx_d of LeabraLayerSpec
Maximum deviation from initial i_kwta_pt allowed (as proportion of initial)
@item l
@vindex l of LeabraLayerSpec
Proportion of difference from target activation to allocate to the leak in G_BAR_IL mode
@item a_dt
@vindex a_dt of LeabraLayerSpec
Time constant for integrating average average activation, which is basis for adapting i_kwta_pt
@end table

@item clamp
@vindex clamp of LeabraLayerSpec
How to clamp external inputs.

@table @code
@item hard
@vindex hard of LeabraLayerSpec
Whether to hard clamp external inputs
    to this layer (directly set activation, resulting in much faster
    processing), or provide external inputs as extra net input (soft
    clamping, if false).
@item gain
@vindex gain of LeabraLayerSpec
tarting soft clamp gain factor (net = gain * ext).
@item d_gain
@vindex d_gain of LeabraLayerSpec
For soft clamp, delta to increase gain
    when target units not >.5 (0 = off, .1 std when used).
@end table
  
@item decay
@vindex decay of LeabraLayerSpec
Proportion of decay of activity state vars between
  various levels of processing:

@table @code
@item event
@vindex event of LeabraLayerSpec
Decay between different events.
@item phase
@vindex phase of LeabraLayerSpec
Decay between different phases.
@item phase2
@vindex phase2 of LeabraLayerSpec
Decay between 2nd set of phases (if applicable). 
@item clamp_phase2
@vindex clamp_phase2 of LeabraLayerSpec
If true, hard-clamp second plus phase activations to prev plus phase
(only special layers will then update -- optimizes speed).
@end table

@item layer_link
@vindex layer_link of LeabraLayerSpec
Link inhibition between layers (with specified gain), rarely used.
Linked layers are in layer objects.
  @code{link} = Whether to link the inhibition, 
  @code{gain} = Strength of the linked inhibition.
@end table

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-proc, leabra-stats, leabra-layer, leabra
@section Leabra Proceses
@cindex Process, Leabra
@tindex LeabraCycle
@tindex LeabraSettle
@tindex LeabraTrial

The core set of Leabra processes consist of a @b{LeabraTrial} process that
performs the two phases of contrastive Hebbian learning (CHL), where
each phase of settling is controlled by a @b{LeabraSettle} process, which in
turn iterates over a number of individual @b{LeabraCycle} processing steps,
each of which updates the activation state of the network.  These
processes fit nicely within the standard settling and cycle processes
(@pxref{proc-levels}).

The @b{LeabraTrial} process iterates over two loops of settle processing,
one for each phase.  It has the following variables:
@table @code
@item Counter phase_order
@vindex phase_order of LeabraTrial
Different orders of phases can be presented, as indicated by the
relatively self-explanatory options.  The @code{MINUS_PLUS_NOTHING}
option allows the network to learn to reconstruct its input state by
taking away any external inputs in an extra third phase, and using
this as a minus phase relative to the immediately preceding plus
phase.  The @code{MINUS_PLUS_PLUS} is used by more complex
working-memory/context algorithms for an extra step of updating of
context reps after the standard minus-plus learning phases.  Note that
the @b{PhaseOrderEventSpec} can be used to set the phase order on a
trial-by-trial basis.
@item Counter phase_no
@vindex phase_no of LeabraTrial
The counter for this process, it goes from 0 to 1 for the two
different phases (or 2 for more phases).
@item Phase phase
@vindex phase of LeabraTrial
The phase the process is in, which is just a more readable
version of the counter:  @code{MINUS_PHASE} and @code{PLUS_PHASE}.
@item StateInit trial_init
@vindex trial_init of LeabraTrial
Indicates what to do at the start of each trial process.
Typically, one wants to @code{DECAY_STATE}, but it is also possible to
@code{INIT_NOTHING} or @code{DO_NOTHING}.  Decay state allows for
working memory across trials, as needed by several modifications to
Leabra.  The default decay parameters decay 100%, which makes it
equivalent to INIT_STATE.
@item bool no_plus_stats
@vindex no_plus_stats of LeabraTrial
This flag means that statistics will not be recorded in the plus phase.
This is useful because squared error, for example, is only meaningful in
the minus phase, since the correct answer is clamped in the plus phase.
@item bool no_plus_test
@vindex no_plus_test of LeabraTrial
This flag means that the plus phase will not be run if the epoch process
indicates that it is in @code{TEST} mode (i.e., no learning is taking
place).
@end table

@vindex min_cycles of LeabraSettle
@vindex netin_mod of LeabraSettle
@vindex send_delta of LeabraSettle
The @b{LeabraSettle} process iterates over cycles of settling
(activation updating).  @code{min_cycles} ensures that at least this
many cycles of processing occur.  It also contains several important
variables that control how activations are computed during settling.
@code{netin_mod} allows one to skip computing the net input every
other cycle (or more), which can result in more efficient computation
by allowing the membrane potentials to keep up better with netinput
changes, but values higher than 2 are not recommended and have
resulted in worse overall learning performance.  @code{send_delta}
is @b{very important for large networks} -- it results in
substantially faster processing by only sending netinput when the
sending activation @emph{changes}, instead of sending it all the
time.  The amount of change that is required is specified in the unit
spec @code{opt_thresh} params.

@cindex DurEvent
The settle process will use the @code{duration} field from a @b{DurEvent}
to set the max number of cycles to settle on a given event.

The @b{LeabraCycle} process updates the activation state of the
network.  It has no user-settable parameters.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-stats, leabra-defs, leabra-proc, leabra
@section Leabra Statistics
@cindex Statistics, Leabra

There are several statistics which are specific to the Leabra package,
including one that compute the global "goodness" (aka energy) of the
network's activation state (@b{LeabraGoodStat}), another set of statistics
for measuring the probabilities of different activation states
(@b{LeabraDistStat, LeabraTIGstat, LeabraTargStat}), and a statistic that can be
used to control the length of settling based on the amount of activation
change (@b{LeabraMaxDa}).

@menu
* leabra-stats-good::               The Goodness (Energy) Statistic
* leabra-stats-maxda::              Measuring the Maximum Delta-Activation
@end menu

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-stats-good, leabra-stats-maxda, leabra-stats, leabra-stats
@subsection The Goodness (Energy) Statistic
@tindex LeabraGoodStat
@cindex Harmony
@cindex Stress
@cindex Goodness

The @b{LeabraGoodStat} computes the overall goodness of the activation
state, which is composed of two terms, the @emph{harmony} and @emph{stress}.
Harmony reflects the extent to which the activations satisfy the
constraints imposed by the weights.  It is just a sum over all units of
the product of the activations times the weights:

@iftex
@tex
% html H = SUM_j SUM_i a_j w_ij a_i
$$   H = \sum_j \sum_i a_j w_{ij} a_i $$	
@end tex
@end iftex
@ifinfo
     H = SUM_j SUM_i a_j w_ij a_i
@end ifinfo

The stress term reflects the extent to which unit's activations are
"stressed" at their extreme values.  It is just the inverse sigmoidal
function of the unit activation values:

@iftex
@tex
% html S = SUM_j f^-1(a_j) 
$$   S = \sum_j f^{-1}(a_j) $$	
@end tex
@end iftex
@ifinfo
     S = SUM_j f^-1(a_j) 
@end ifinfo

The total goodness is just the harmony minus the stress. These values
are stored in the @code{hrmny}, @code{strss}, and @code{gdnss} stat
value members of the goodness stat.  The net input to a unit is used
for computing the harmony term, since harmony is just the unit's
activation times its net input.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-stats-maxda,  , leabra-stats-good, leabra-stats
@subsection Measuring the Maximum Delta-Activation
@tindex LeabraMaxDa

The @b{LeabraMaxDa} statistic computes the maximum delta-activation (change
in activation) for any unit in the network.  This is used to stop
settling once the network has reached equilibrium.  The stopping
criterion for this statistic is the tolerance with which equilibrium is
measured.  It is created by default in the @b{LeabraSettle} process.

It can use the change in net current (@code{INET}) in addition to
actual activation change (@code{da}) so as to not trigger a false
alarm based on sub-treshold activations not changing (their net
currents can be changing even if the activations are not).  Once the
layer average activation goes over @code{lay_avg_thr}, the stat
switches over to measuring @code{da} instead of inet.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-defs, leabra-misc, leabra-stats, leabra
@section Leabra Defaults
@cindex Defaults, Leabra
@cindex Leabra, Defaults

The following default files (@pxref{proj-defaults}) are available for
configuring different versions of the Leabra objects:

@table @file
@item leabra.def
This is the standard defaults file.
@item leabra_seq.def
Sequence-based processes for doing sequences of events within event groups.
@item leabra_ae.def
An auto-encoder configuration for doing MINUS_PLUS_NOTHING kind of
learning automatically.
@item leabra_seq_ae.def
Sequence plus auto-encoder.
@item leabra_seq_time.def
Uses LeabraTimeUnit units that can learn based on temporal differences
across sequential trials.  This never worked very well and is
superceeded by various context memory approaches.
@item leabra_seq_ae_time.def
Sequences plus auto-encoder plus time-based units!
@end table

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-misc, leabra-impl, leabra-defs, leabra
@section Leabra Misc Special Classes

@tindex LeabraWiz
@cindex Wizard, Leabra
Leabra has a number of special derived classes for doing special
things beyond the standard learning mechanisms.  These are not well
documented here -- interested users should refer to the source code.
The @b{LeabraWiz} wizard object has some specialized functions for
creating some of these classes, and can also setup unit-based
inhibition in the network.

@cindex Reinforcement Learning
@cindex Temporal Differences (TD)
@cindex Adaptive Critic
@tindex LebraACLayerSpec
The @b{LebraACLayerSpec} implements an Adaptive Critic for performing
Temporal Differences reinforcement learning.  This implementation is
not particularly good relative to the new improved
@b{RewPredLayerSpec}, but it is simpler and is widely used.

@cindex Simple Recurrent Networks
@tindex LeabraContextLayerSpec
The @b{LeabraContextLayerSpec} implements a Simple Recurrent Network
context layer, and can be constructed automatically using the
@b{LeabraWiz}.  The fancier @b{LeabraGatedCtxLayerSpec} takes gating
control signals from an AC (adpative critic) unit uses them to
control the rate of context updating.  The @b{LeabraACMaintLayerSpec}
is another version of this idea, which uses intrinsic maintenance
currents (i.e., hysteresis currents) to maintain information in the
context layer over time, as modulated by the AC signal.  See
@cite{Rougier & O'Reilly, 2002} for further documentation on this
mechanism.

@tindex PhaseOrderEventSpec
The @b{PhaseOrderEventSpec} allows for an event to control its own set
of phases that it will use.

@tindex LeabraTimeConSpec
@tindex LeabraTimeUnit
@tindex LeabraTimeUnitSpec
The @b{LeabraTimeConSpec}, @b{LeabraTimeUnit}, and
@b{LeabraTimeUnitSpec} implement learning across two adjacent events.
The units store prior trial minus and plus phase activations
(@code{p_act_m} and @code{p_act_p}) and use these for learning.

@tindex LeabraNegBiasSpec
The @b{LeabraNegBiasSpec} makes the bias weight only learn based on
negative error derivatives (i.e., the bias weight can only go
negative).  The @code{decay} parameter allows this negative bias to
recover back towards zero.  This is useful for implementing a simple
form of search, where things that produce errors are made less likely
to be activated in the near future.

@tindex LeabraTabledConSpec
The @b{LeabraTabledConSpec} allows for learning to be driven by a
lookup table, e.g., for exploring biologically-derived learning rules
that do not have a simple analytic form.

@tindex ScalarValLayerSpec
The @b{ScalarValLayerSpec} implements a layer that represents a single
scalar value usign a coarse-coded representation, where each unit has
a designated target value, and it ``votes'' for this value in
proportion to its activation strength.  The overall coded value is the
weighted average of these target values times the unit activations.
The first unit in the layer contains a convenient readout of the
represented scalar value, and is otherwise prevented from
participating in normal network updating.  Clamping a value to this
first unit causes the rest of the units to be clamped into
Gaussian-shaped bump representing that value.

@tindex MarkerConSpec
The @b{MarkerConSpec} is useful for marking special connections that
operate according to non-standard functions.  It turns off learning
and does not contribute to normal netinput computations.

@tindex TdModUnit
@tindex TdModUnitSpec
The @b{TdModUnit} and @b{TdModUnitSpec} add a temporal-differences
modulation value to the unit variables.  They also include the
time-based variables found in the Time classes.  These are used in the
PFC/BG learning classes.

@tindex RewPredLayerSpec
The @b{RewPredLayerSpec} is an improved version of the AC unit, which
is based on the ScalarValLayerSpec, so it produces a coarse-coded
representation of reward expectation.  It also deals with
non-absorbing rewards much better.

@tindex PatchLayerSpec
@tindex MatrixLayerSpec
@tindex MatrixUnitSpec
@tindex ImRewLayerSpec
@tindex SNcLayerSpec
@tindex PFCLayerSpec

The following specs implement an experimental version of
dynamically-gated working memory based on the biology of the
prefrontal cortex (PFC) and basal ganglia (BG).  See the
@code{HelpConfig} button for more specific information.
@b{PatchLayerSpec}, @b{MatrixLayerSpec}, @b{MatrixUnitSpec},
@b{ImRewLayerSpec}, @b{SNcLayerSpec}, @b{PFCLayerSpec}.

@c ======================================
@c    <node>, <next>, <prev>, <up>
@node  leabra-impl,  , leabra-misc, leabra
@section Leabra Implementational Details

TBW.  Sorry.


