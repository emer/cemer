For parallel execution, we need to develop some constructs that
will enable us to efficiently and cleanly make use of multi-cores
and auxiliary computing resources.

Issues

* need to separate the mechanism of iteration from the algorithm
  -- most Network steps can then become threadable simply by
  having a dispatcher call several copies 
* practical point: EVERYTHING we iterate is either in a List
  or a Group
* the algos in Emergent are byzantine, and the specialized ones
  often pass in three or more parameters of context, such as
  LeabraLayerSpec, LeabraConSpec, etc.; the call chains are also
  byzantine, often passing through the obj to the spec
* some of the routines have this structure:
   // do some setup
   // iterate
   //   call item fun(item, setup stuff)
  -- this is very thread-unfriendly, and also iteration unfriendly,
     UNLESS we can have a simple way to indicate the first vs.
     subsequent calls
  
Notes:

* cooperative multi-tasking/threading will require *restartable*
  iterating, which implies that the iteratee cannot do the entire
  thing
* in order to insure algos don't need to know about multi-t/t the
  easiest way might be to just pass them a small part of the allotment
* implies an Itr/Vistr with at least i_fm and i_to capability
* the iterator *capabilities* are limited to every routine respecting
  them; therefore, ex. no point having "by" unless every routine uses
  that; also, iteration may be more efficient by 1
* separating the collection originator from the client gives MUCH
  more flexibility; ex. the client routine doesn't need to know if
  it is driven off layer.units or net.all_units
* all "virtual" or "object" routines are recast statically as --
  this lets them be easily passed as function pointers:

Object
  void SomeRoutine(...)
  S_SomeRoutine(Object* inst, ...)
  
Approaches

1) Top level dispatcher
   Central level generic iterator dispatcher
   Core routines, accept (Iterator)
   
2) Top level dispatcher
   Central level generic Visitor
   Core routines accept visitor

"Local (Item)" Routines

Send_Netin(Unit*)
Compute_SRAvg(Unit*)

*Dispatch' routines

Network::Compute_Netin
Network::Compute_SRAvg() // note: can be semi-async
Network::Compute_dWt()
Network::Compute_Weight()

Pros/Cons
Iterator approach
* routines need to be iterator-aware, take itr as a param
Visitor approach
* there really aren't any "middle" routines -- just a ptr to
  an inner routine, and a Visitor on the collection

Isn't the issue related just to the middle level, the one
that iterates? In both cases:
* inner routines are indifferent

What about doing this tasking/threading? 

Jobs

Let us define computation as a set of Jobs. A Job is a clearly
defined piece of work to be done. One thing to note is that
Jobs do not need to be sequential or non-overlapping. They thus
do not need to be in the same time frame. For example, an
entire process may comprise a series of overlapping jobs at
different time scales.

Synchronization

Some Jobs have temporal relations--this is similar to Project
management, wherein Job B may not be able to begin until Job A
has completed.

Example

Here are the names of jobs:

SR - SendRecv Average
N - Netin
C - per Cycle items (stats, etc.)
d - dWt
W - Weight
S - per Settle items

      SR       |
N-C-N-C-N-C-N-C| repeat 9 times |N-d-W-S

We can maximize parallelism by the following:
* minimize hard sync points
* have overlapping jobs at different time scales

Transparent Synchronization

If you call a routine, that does all its work before completing,
then you can still check if it is complete, without knowing that
you had finished for sure.

Niterating (N-way Iterating)

We will concern ourself only with work that can be 
iterated using a sequential bounded index. To balance
caching and bus locking (for atomic operations) we can
define an iteration primitive:

int& g_i -- reference to the global iter counter
  MUST start at 0 (or %I_BY) in this scheme
int  i_to -- maximum i value
int  I_BY -- (up to) this many items per block (2^n)
int  M_BY -- bit mask for the 2^n, derived from I_BY

canonical iteration loop
(note: x ? y : z  will ONLY eval y OR z)

for (
  int my_i = AtomicFetchAdd(&g_i, I_BY);
  my_i <= i_to;
  my_i = (++my_i & M_BY) ? my_i : AtomicFetchAdd(&g_i, I_BY)
) {
  // do work here
}



Note that I_BY (and its derivative M_BY) can be implicit
constants built in to the client routines (or global 
constants or literals) thus their capitalization. So the
only values needed by the client code are g_i and i_to. Also,
dispatch code can eliminate most locking overhead by