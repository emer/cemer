Threading and Clustering Notes


Need to be able to efficiently parcel the computation between threads, processors, and nodes.

Issues:

* calculation load
* internode communication
* latency
* syncronization
* dead time
* "slowest throttles"

Assumptions:

* each node is relative asynchronous, and may experience short-term fluctuations in capacity (due to allocation of overhead by OS to each thread)

If calculation is not distributed equally, to each node based on that node's relative capacity, then some nodes will do more work. If the process is synchronous, then the slowest nodes will dominate the overall throughput.

The thing that will most slow us up is needing to wait on everyone else, before we can continue. We need a pipelining method, whereby we can calculate our piece, send the result, and then start on our next piece without waiting for the other results.

Major phases:

* net input
* activation
* kwta inhibition
* learning

What if we had a two-phase system, whereby the first phase was an aproximation, and the second phase calculated the error from phase 1, and corrected the system.

Some assumptions:

* there is inertia -- next value will typically be similar to last value, in many cases
* d's are available -- we can know the change in input from last time, to do a prediction of future values


Semi-manual Partitioning

In a large system with realistic i/o, there will be much parallelism that can probably be manually partitioned. ex. image preprocessing, cells in a V1 system, cells in a M1 system, etc.

Maybe it is best to have many parts quasi-synchronously working on small chunks. 
"quasi-synchronous" -- means overall working in concert, at the same rate, but at a micro scale, they are not synchronized, and the date may not be synchronized; ex. a routine might be reading its input values, and some may get updated from previous stage part way through.


Predictive-Corrective Synchronism

One major issue is the recurrent architecture -- you need both downstream and upstream, but you are feeding upstream. Maybe we could factor the upstream into a direct linear feedback-with-delay, and a correction component that will come from upstream.

* a node gets a subset of the entire network
* a node will have units that it computes, and units that are its inputs (it won't have or need
  to have the entire network)
* a unit may either be computed by a node, or tracked by the node
* the current value of a tracked node is predicted
* the node always has the previously predicted, and actual value
* each cycle in which a predicted value is used, the following is done:
  - apply correction factor based on the error of last prediction
  - predict new value

Correction Factor:

The correction factor will have to take at least three things into account:

* non-linearity of the i/o transfer function
  - although it is non-linear, it is monotonic, and there always exists
    a solution to the question of what the output should be to 
    correct for the known error 
* time -- a correction applied at t(n+1) is not the same as the same 
  value applied at t(n)
  - the system *does* integrate the inputs, and the integration factor
    is known, so the correct value can be determined to make the result
    seem as though it occurred last time
* side-effects, layer-effects -- things like kwta are computed in an 
  overall way -- it will not be easy to correct these directly
  - *however* things like inhibition are generalized, and based on
    the large scale value, and are probably not severely affected by
    small errors in the actual values -- this could be tested by adding
    noise equivalent to the expected errors
    
    
Dynamic Allocation

The entire simulation will be limited in throughput to that of the slowest node. It is 
therefore critical to partition the problem as equally as possible across the nodes    
* we do not insist that 
assume the following:
* we either have an "exact" value, or a fairly good prediction available for input values
* our processing routine has the property that it either rarely uses only predictions;
  or else it proceeds someways randomly or non-predictively or such, so that it doesn't get
  locked into always choosing the same exact and predicted values
* it would be great if it could be weighed to try to get exact values, and "good" predictions,
  and only hit "bad" or "uncertain" predictions as little as possible
  
Delta

There are basically 3 different mechanisms:
* clamp net
  clmp_net input is its own thing, always added to net
  the Send for hard_clamped layers is done once, 
* normal act/net
* act_delta, act_sent, net_delta

Hard Clamped
* independent of delta mode
* always used (not conditional)
* has its own dedicated net target (clmp_net)
* can only be used for excit (error detect if attempt with inhib)
* lay.hard_clamped is set in:
  in Settle_Init:
    Compute_HardClamp
     clamp.hard && (lay->ext_flag & Unit::EXT)
    (also, for 3-phase in phase >2: Compute_HardClampPhase2)
* then sent later in Settle_Init
  Send_ClampNet (if hard_clamped set)
* clmp_net is cleared in:
  Init_ClampNet
  called from Network::Send_ClampNet
  
  
Some key routines:
  Network::SettleInit
    Compute_HardClamp -- computes hard clamp, and sets lay.hard_clamped
    Send_ClampNet -- clears all clmp_net, and sends hard_clamped ones
      Init_ClampNet -- clears ALL un.clmp_net values
  